#!/usr/bin/env python3
"""
===================== lat-plots.py ======================
A semi-official place to put plots of interest, primarily
using the "final" LAT cut files generated by lat-expo.py
=================== C. Wiseman (USC) ===================
"""
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('./clint.mpl')
from matplotlib.colors import LogNorm

import dsi
bkg = dsi.BkgInfo()
det = dsi.DetInfo()
cal = dsi.CalInfo()
import waveLibs as wl
import tinydb as db

def main():

    # a very important parameter, use 90 or 95
    global pctTot
    # pctTot = 90
    pctTot = 95 # <-- use this one
    # print("Using pctTot ==",pctTot)

    # spec()
    # spec_vs_cpd()
    # spec_summary()
    # thresh_cut_cal()
    # ds3_det_eff()
    # hi_mult_cal_spec()
    # m2s238_sum_peak()
    # m2s238_hit_spec()
    # save_slowness_data()
    # slowness_vs_energy()
    # fitSlo_stability()
    # fitSlo_distribution()
    # fitSlo_det_efficiencies()
    # fitSlo_tot_efficiencies()
    # fitSlo_exposure_weighted_eff()
    # get_ext_pulser_data()
    # plot_ext_pulser()
    # plot_tOffset()
    # plot_riseNoise()
    fitSlo_efficiency_uncertainty()
    # rateCheckDS()
    # rateCheckDet()
    # rateSummary()
    check_spec()


def spec():
    from ROOT import TFile, TChain, TTree

    dsList = [0,1,2,3,4,"5A","5B","5C"]

    tt = TChain("skimTree")
    enrExp, natExp = 0, 0
    for ds in dsList:
        inFile = "%s/bkg/cut/final%d/final%d_DS%s.root" % (dsi.dataDir, pctTot, pctTot, ds)
        tf = TFile(inFile)
        enrExp += float(tf.Get("enrExp (kg-d)").GetTitle())
        natExp += float(tf.Get("natExp (kg-d)").GetTitle())
        tf.Close()
        tt.Add(inFile)

    print("enrExp %.2f  natExp %.2f, ds" % (enrExp/365.25, natExp/365.25),dsList)

    fig = plt.figure(figsize=(8,7))
    p0 = plt.subplot(211)
    p1 = plt.subplot(212)
    xLo, xHi, xpb = 0, 20, 0.1

    # natural
    tCut = "!isEnr"
    n = tt.Draw("trapENFCal",tCut,"goff")
    hitE = tt.GetV1()
    hitE = [hitE[i] for i in range(n)]
    x, h1 = wl.GetHisto(hitE, xLo, xHi, xpb, shift=False)
    h1 = np.divide(h1, natExp*xpb) # scale by exposure and binning
    p0.plot(x, h1, 'b', ls='steps', lw=2, label="Natural")
    # p0.set_xlabel("Energy (keV)", ha='right', x=1)
    p0.set_ylabel("Counts/keV-kg-d", ha='right', y=1)
    p0.legend(loc=1)

    # enriched
    tCut = "isEnr"
    n = tt.Draw("trapENFCal",tCut,"goff")
    hitE = tt.GetV1()
    hitE = [hitE[i] for i in range(n)]
    x, h1 = wl.GetHisto(hitE, xLo, xHi, xpb, shift=False)
    h1 = np.divide(h1, enrExp*xpb) # scale by exposure and binning
    p1.plot(x, h1, 'b', ls='steps', lw=2, label="Enriched")
    p1.set_xlabel("Energy (keV)", ha='right', x=1)
    # p1.set_ylabel("Counts/keV-kg-d", ha='right', y=1)
    p1.legend(loc=1)

    plt.tight_layout()
    # plt.show()
    plt.savefig("./plots/lat-final%d-specFinal.pdf" % pctTot)


def spec_vs_cpd():
    from ROOT import TFile, TChain, TTree

    dsList = [0,1,2,3,4,"5A","5B","5C"]

    tt = TChain("skimTree")
    enrExp, natExp = 0, 0
    for ds in dsList:
        inFile = "%s/bkg/cut/final%d/final%d_DS%s.root" % (dsi.dataDir, pctTot, pctTot, ds)
        tf = TFile(inFile)
        enrExp += float(tf.Get("enrExp (kg-d)").GetTitle())
        natExp += float(tf.Get("natExp (kg-d)").GetTitle())
        tf.Close()
        tt.Add(inFile)
    print("enrExp %.2f  natExp %.2f, ds" % (enrExp/365.25, natExp/365.25),dsList)

    # the channel map changes across datasets, so we have to plot by CPD
    cpdList = []
    for ds in dsList:
        dsTmp = int(ds[0]) if isinstance(ds,str) else ds
        chTmp = det.getGoodChanList(dsTmp)
        for ch in chTmp:
            cpdList.append(int(det.getChanCPD(dsTmp,ch)))
    cpdList = sorted(list(set(cpdList)))

    enrList = [cpd for cpd in cpdList if det.allDetIDs[str(cpd)] > 100000]
    cpdEnrMap = {enrList[i]:i for i in range(len(enrList))}
    # enrLabels = enrList

    natList = [cpd for cpd in cpdList if det.allDetIDs[str(cpd)] < 100000]
    cpdNatMap = {natList[i]:i for i in range(len(natList))}
    # natLabels =

    fig = plt.figure(figsize=(8,7))
    p0 = plt.subplot(211)
    p1 = plt.subplot(212)
    xLo, xHi, xpb = 0, 20, 0.2

    # natural
    tCut = "!isEnr"
    yLo, yHi = 0, len(natList)
    nbx, nby = int((xHi-xLo)/xpb), len(natList)

    n = tt.Draw("trapENFCal:C:P:D",tCut,"goff")
    hitE, hitC, hitP, hitD = tt.GetV1(), tt.GetV2(), tt.GetV3(), tt.GetV4()
    hitE = [hitE[i] for i in range(n)]
    hitCPD = [cpdNatMap[ int("%d%d%d" % (hitC[i],hitP[i],hitD[i])) ] for i in range(n)]

    hNat,_,_,im0 = p0.hist2d(hitE, hitCPD, bins=[nbx, nby], range=[[xLo,xHi],[yLo,yHi]], cmap='jet')
    # p0.set_xlabel("Energy (keV)", ha='right', x=1.)
    p0.set_xticks(np.arange(xLo, xHi+1, 1.0))
    p0.set_ylabel("CPD, Natural", ha='right', y=1.)
    p0.set_yticks(np.arange(0, len(natList))+0.5)
    p0.set_yticklabels(natList, fontsize=8)

    # enriched
    tCut = "isEnr"
    yLo, yHi = 0, len(enrList)
    nbx, nby = int((xHi-xLo)/xpb), len(enrList)

    n = tt.Draw("trapENFCal:C:P:D",tCut,"goff")
    hitE, hitC, hitP, hitD = tt.GetV1(), tt.GetV2(), tt.GetV3(), tt.GetV4()
    hitE = [hitE[i] for i in range(n)]
    hitCPD = [ cpdEnrMap[int("%d%d%d" % (hitC[i],hitP[i],hitD[i])) ] for i in range(n)]

    hEnr,_,_,im1 = p1.hist2d(hitE, hitCPD, bins=[nbx, nby], range=[[xLo,xHi],[yLo,yHi]], cmap='jet')
    p1.set_xlabel("Energy (keV)", ha='right', x=1.)
    p1.set_xticks(np.arange(xLo, xHi+1, 1.0))
    p1.set_ylabel("CPD, Enriched", ha='right', y=1.)
    p1.set_yticks(np.arange(0, len(enrList))+0.5)
    p1.set_yticklabels(enrList, fontsize=8)

    cb0 = fig.colorbar(im0, ax=p0)
    cb1 = fig.colorbar(im1, ax=p1)
    # cb0.set_label('cts', ha='right', rotation=270, labelpad=20)


    plt.tight_layout()
    # plt.show()
    plt.savefig("./plots/lat-spec-vs-cpd-e%d.pdf" % pctTot)

    # ========= get detector counts =========

    eLo, eHi = 10, 11
    bLo, bHi = int(eLo/xpb), int(eHi/xpb)

    print("Natural Counts, %d-%d keV" % (eLo, eHi))
    for cpd in natList:
        col = cpdNatMap[cpd]
        print(cpd, int(np.sum(hNat[bLo:bHi,col])))

    print("Enriched Counts, %d-%d keV" % (eLo, eHi))
    for cpd in enrList:
        col = cpdEnrMap[cpd]
        print(cpd, int(np.sum(hEnr[bLo:bHi,col])))


def spec_summary(dsList=None, dtype='enr', rateMode=False, eMin=20, eMax=40):
    """ Make separate plots for enr/nat, with the spectrum and efficiency,
    and then a 2d by channel under it. """

    from ROOT import TFile, TChain, TTree

    if eMin < 1:
        print('Cannot currently calculate rates below 1 keV, setting minimum to 1 keV')
        eMin = 1

    if not dsList:
        dsList = [1,2,3,4,"5A","5B","5C",6]

    xLo, xHi, xpb = 0, 200, 0.2

    tt = TChain("skimTree")
    enrExp, natExp = 0, 0
    # Change exposures to grab from the output file rather than from ROOT files
    for ds in dsList:
        inFile = "%s/bkg/cut/final%dt/final%dt_DS%s.root" % (dsi.dataDir, pctTot, pctTot, ds)
        tf = TFile(inFile)
        # enrExp += float(tf.Get("enrExp (kg-d)").GetTitle())
        # natExp += float(tf.Get("natExp (kg-d)").GetTitle())
        tf.Close()
        tt.Add(inFile)

    # load efficiency correction
    f = np.load("./data/lat-expo-efficiency-all-e%d.npz" % pctTot)
    xEff = f['arr_0']
    totEnrEff, totNatEff = f['arr_1'].item(), f['arr_2'].item()
    enrExpDict, natExpDict = f['arr_3'].item(), f['arr_4'].item()

    detEff = np.zeros(len(xEff))
    for ds in dsList:
        enrExp += enrExpDict[ds]
        natExp += natExpDict[ds]
        if dtype=="enr": detEff += totEnrEff[ds]
        if dtype=="nat": detEff += totNatEff[ds]

    if dtype=="enr":
        detExp = enrExp
        specLabel = "Enriched"
    if dtype=="nat":
        detExp = natExp
        specLabel = "Natural"

    if not rateMode: print("%s Exp: %.2f, ds" % (specLabel, detExp/365.25), dsList)

    # normalize the efficiency
    effNorm = (np.amax(detEff)/365.25) / (detExp/365.25)
    detEff = effNorm * np.divide(detEff, np.amax(detEff))
    idxE = np.where((xEff <= xHi) & (xEff >= xLo))
    xEff, detEff = xEff[idxE], detEff[idxE]

    # make the plot!
    fig, (p1, p2) = plt.subplots(2,1, figsize=(8,7))

    # ==== 1. energy spectrum ====

    if dtype=="enr": tCut = "isEnr"
    if dtype=="nat": tCut = "!isEnr"

    n = tt.Draw("trapENFCal",tCut,"goff")
    hitE = tt.GetV1()
    hitE = [hitE[i] for i in range(n)]
    x, hCts = wl.GetHisto(hitE, xLo, xHi, xpb, shift=False)

    hSpec = np.divide(hCts, detExp * xpb) # scale by exposure and binning to get cts/(keV kg d)
    hErr = np.asarray([np.sqrt(hBin/(detExp*xpb)) for hBin in hSpec]) # statistical error in each bin

    # calculate background index rate (smaller error if you use 1 keV bins)
    idxR = np.where((x>=eMin) & (x<=eMax))

    # Only do this in rateMode
    if xHi >= eMax and rateMode:
        hSpecCorr = hSpec[:-1]/detEff[::int(len(detEff)/(len(hSpec)-1))]
        hRate = np.sum(xpb * hSpec[idxR])/(eMax-eMin)
        hRateUnc = np.sqrt(np.sum(np.square(hErr[idxR]))/(eMax-eMin))
        hRateCorr = np.sum(xpb * hSpecCorr[idxR])/(eMax-eMin)
        hRateCorrUnc = np.sqrt(np.sum(xpb*hCts[idxR]))/np.sum(xpb*hCts[idxR])*hRateCorr
        # print("Rate %d-%d: %.5f ± %.5f, %.5f ± %.5f" % (eMin, eMax, hRate, hRateUnc, hRateCorr, hRateCorrUnc))
        print("%d-%d & %.3f $\pm$ %.3f" % (eMin, eMax, hRateCorr, hRateCorrUnc))
        return

    dsLabel = "DS%s--%s" % (dsList[0], dsList[-1]) if len(dsList) > 1 else "DS%s" % dsList[0]
    p1.plot(x, hSpec, 'b', ls='steps', lw=2, label="%s, %s" % (specLabel, dsLabel))

    p1.plot(np.nan, np.nan, '--r', alpha=0.8, lw=2, label="Eff, %.2f%% at %d keV" % (effNorm*100, xHi))
    p1.axvline(1, c='g', lw=1, alpha=0.8, label="1.0 keV")
    # p1.axvline(1.5, c='m', lw=1, alpha=0.8, label="1.5 keV")

    p1.set_xlim(xLo, xHi)
    p1.set_xticks(np.arange(xLo, xHi+1, (xHi-xLo)/10))
    # p1.set_xlabel("Energy (keV)", ha='right', x=1)
    p1.set_ylabel("Counts/keV-kg-d", ha='right', y=1)
    p1.legend(loc=1, fontsize=14, bbox_to_anchor=(0., 0.7, 1, 0.2))

    p1a = p1.twinx()
    p1a.plot(xEff, detEff, '--r', alpha=0.8, lw=2)
    p1a.set_ylabel('Efficiency', color='r', ha='right', y=1)
    p1a.set_yticks(np.arange(0,1.1, 0.2))
    p1a.tick_params('y', colors='r')

    # ==== 2. spectrum by detector ====
    n = tt.Draw("trapENFCal:C:P:D",tCut,"goff")
    hitE, hitC, hitP, hitD = tt.GetV1(), tt.GetV2(), tt.GetV3(), tt.GetV4()
    hitE = [hitE[i] for i in range(n)]
    hitCPD = [int("%d%d%d" % (hitC[i],hitP[i],hitD[i])) for i in range(n)]

    # load exposure of each detector to weight histogram
    # fE = np.load("./data/expo-totals-e%d.npz" % (pctTot))
    # for key in fE: detExpo = fE[key].item()
    # detExpoTot = {cpd:0 for cpd in det.allDets}
    # for cpd in detExpoTot:
    #     for ds in dsList:
    #         detExpoTot[cpd] += detExpo[ds][cpd]
    # hitW = [detExpoTot[str(cpd)] for cpd in hitCPD]

    # the channel map changes across datasets, so we have to plot by CPD
    cpdList = []
    for ds in dsList:
        dsTmp = int(ds[0]) if isinstance(ds,str) else ds
        chTmp = det.getGoodChanList(dsTmp)
        # cpdTmp = sorted([int(det.getChanCPD(dsTmp, ch)) for ch in chTmp])
        # print(ds, cpdTmp)
        for ch in chTmp:
            cpd = det.getChanCPD(dsTmp,ch)
            if dtype=="enr" and det.isEnr(cpd): cpdList.append(int(cpd))
            if dtype=="nat" and not det.isEnr(cpd): cpdList.append(int(cpd))

    cpdList = sorted(list(set(cpdList)))

    rejectedDetectors = [cpd for cpd in cpdList if int(cpd) not in set(hitCPD)]
    print("Rejected detectors:",rejectedDetectors)

    # if we check for the set, we don't plot detectors that have been completely killed by cuts from the good detector list
    if dtype=="enr": cpdList = [cpd for cpd in cpdList if det.isEnr(cpd) and int(cpd) in set(hitCPD)]
    if dtype=="nat": cpdList = [cpd for cpd in cpdList if not det.isEnr(cpd) and int(cpd) in set(hitCPD)]

    cpdMap = {cpdList[i]:i for i in range(len(cpdList))}
    hitCPD = [ cpdMap[int("%d%d%d" % (hitC[i],hitP[i],hitD[i])) ] for i in range(n)]

    yLo, yHi = 0, len(cpdList)
    nbx, nby = int((xHi-xLo)/xpb), len(cpdList)

    # hEnr,_,_,im1 = p2.hist2d(hitE, hitCPD, weights=hitW, bins=[nbx, nby], range=[[xLo,xHi],[yLo,yHi]], cmap='jet')
    hEnr,_,_,im1 = p2.hist2d(hitE, hitCPD, bins=[nbx, nby], range=[[xLo,xHi],[yLo,yHi]], cmap='jet')
    p2.set_xlabel("Energy (keV)", ha='right', x=1.)
    p2.set_xticks(np.arange(xLo, xHi+1, (xHi-xLo)/10))
    p2.set_ylabel("CPD, %s" % specLabel, ha='right', y=1.)
    p2.set_yticks(np.arange(0, len(cpdList))+0.5)
    p2.set_yticklabels(cpdList, fontsize=8)

    # Specially tuned colorbar parameters
    # These numbers probably only work for this specific figure size
    # cax = fig.add_axes([0.91, 0.11, 0.02, 0.35]) # For non-tight_layout
    cax = fig.add_axes([0.895, 0.11, 0.015, 0.39])
    cb1 = fig.colorbar(im1, cax=cax)
    cb1.ax.get_yaxis().labelpad = 0
    cb1.ax.set_ylabel('Counts', rotation=90)
    plt.tight_layout()
    # plt.show()
    # plt.savefig("./plots/lat-final%d-%s-DS%s-%d.pdf" % (pctTot, dtype, ''.join([str(d) for d in dsList]), xHi))


def thresh_cut_cal():
    """ Adapted from LAT/sandbox/mult2.py, and dependent on input:
    "./data/mult2-dtVals-ene.npz"
    """
    import tinydb as db
    dsNum, bkgIdx = 5, 83
    calDB = db.TinyDB('./calDB.json')
    pars = db.Query()
    thD = dsi.getDBRecord("thresh_ds%d_bkgidx%d" % (dsNum, bkgIdx), False, calDB, pars)
    det = dsi.DetInfo()

    f = np.load("./data/mult2-dtVals-ene.npz")
    dtVals = f['arr_0'].item()
    mH = 1
    n = len(dtVals[mH])

    # plot 1: all hits under 10 keV
    # chan = [dtVals[mH][i][1][0] for i in range(n) if dtVals[mH][i][2][0]<10]
    # hitE = [dtVals[mH][i][2][0] for i in range(n) if dtVals[mH][i][2][0]<10]
    chan = [dtVals[mH][i][1][0] for i in range(n) if dtVals[mH][i][1][0] not in [598,1172] and dtVals[mH][i][2][0]<10]
    hitE = [dtVals[mH][i][2][0] for i in range(n) if dtVals[mH][i][1][0] not in [598,1172] and dtVals[mH][i][2][0]<10]

    # plot 2: only take hits above the detector threshold.
    n = len(hitE)
    chanTh, hitETh = [], []
    for i in range(n):
        if chan[i] in thD.keys() and hitE[i] > thD[chan[i]][0] + 3*thD[chan[i]][1]:
            chanTh.append(chan[i])
            hitETh.append(hitE[i])

    # make the figure

    # f = plt.figure(figsize=(10,6)) # thesis
    f = plt.figure(figsize=(15,5)) # defense
    p1 = plt.subplot(121)
    p2 = plt.subplot(122)

    chList = list(sorted(set(chan)))

    cpd = {ch : int(det.getChanCPD(dsNum, ch)) for ch in chList}
    cpdList = sorted([int(det.getChanCPD(dsNum, ch)) for ch in chList])
    cpdIdx = {cpdList[i]:i for i in range(len(cpdList))}

    cpdData = [cpdIdx[cpd[ch]] for ch in chan]
    cpdDataTh = [cpdIdx[cpd[ch]] for ch in chanTh]

    xLo, xHi, xpb = 0.5, 5., 0.1
    yLo, yHi = 0, len(chList)
    nbx, nby = int((xHi-xLo)/xpb), len(chList)



    h1,xedg1,yedg1 = np.histogram2d(hitE, cpdData, bins=[nbx,nby], range=[[xLo,xHi],[yLo,yHi]])
    h2,xedg2,yedg2 = np.histogram2d(hitETh, cpdDataTh, bins=[nbx,nby], range=[[xLo,xHi],[yLo,yHi]])
    h1, h2 = h1.T, h2.T

    hMin, hMax = np.amin(h1), np.amax(h2)
    hMin, hMax = 2, hMax*1.3
    h1[h1 < 0.1] = -1
    h2[h2 < 0.1] = -1

    im1 = p1.imshow(h1,cmap='jet',vmin=hMin,vmax=hMax, aspect='auto')#,norm=LogNorm())

    xticklabels = ["%.1f" % x for x in np.arange(0, 5.5, 0.5)]

    yticks = np.arange(0, len(cpdList))

    p1.tick_params(axis='y',which='minor',left='off')
    p2.tick_params(axis='y',which='minor',left='off')


    p1.set_xlabel("Energy (keV)", ha='right', x=1.)
    # p1.set_xticks(xticks)
    p1.set_xticklabels(xticklabels)
    p1.set_ylabel("CPD", ha='right', y=1.)
    p1.set_yticks(yticks)
    p1.set_yticklabels(cpdList, fontsize=10)
    cb1 = f.colorbar(im1, ax=p1)#, fraction=0.037, pad=0.04)
    # cb1.set_label('Counts', ha='right', rotation=270, labelpad=20)

    im2 = p2.imshow(h2,cmap='jet',vmin=hMin,vmax=hMax, aspect='auto')#,norm=LogNorm())
    p2.set_xlabel("Energy (keV)", ha='right', x=1.)
    # p2.set_xticks(xticks)
    p2.set_xticklabels(xticklabels)
    p2.set_ylabel("CPD", ha='right', y=1.)
    p2.set_yticks(yticks)
    p2.set_yticklabels(cpdList, fontsize=10)
    cb2 = f.colorbar(im2, ax=p2)#, fraction=0.037, pad=0.04)
    # cb2.set_label('Counts', ha='right', rotation=270, labelpad=20)

    plt.tight_layout()

    # plt.show()
    plt.savefig("./plots/lat-dtm1-thresh.pdf")


def ds3_det_eff():
    """ Adapted from lat-expo.py::getEfficiency.
    Plots the combined trigger efficiency from each detector in DS3 separately.
    Also plots the slowness efficiency separately
    """
    import tinydb as db
    import lat3
    from ROOT import TFile, TTree
    import matplotlib.pyplot as plt
    plt.style.use('./clint.mpl')

    calDB = db.TinyDB('%s/calDB-v2.json' % (dsi.latSWDir))
    pars = db.Query()
    enrExc, natExc,_,_ = lat3.getOutliers(verbose=False, usePass2=False)

    # mode = "trig"  # trigger efficiency only
    mode = "all"   # does all PS

    # dsList = [0,1,2,3,4,"5A","5B","5C"]
    dsList = [3]

    # efficiency output
    xLo, xHi = 0, 30
    xEff = np.arange(xLo, xHi, 0.01)
    totEnrEff = {ds:np.zeros(len(xEff)) for ds in dsList}
    totNatEff = {ds:np.zeros(len(xEff)) for ds in dsList}
    detEff = {ds:{} for ds in dsList}
    for ds in dsList:
        detEff[ds] = {cpd:np.zeros(len(xEff)) for cpd in det.allDets}

    # recalculate these, make sure they match getExposure
    enrExp = {ds:0 for ds in dsList}
    natExp = {ds:0 for ds in dsList}

    # 1. loop over datasets
    for ds in dsList:

        # set DS stuff
        dsNum = int(ds[0]) if isinstance(ds,str) else ds
        nBkg = bkg.dsMap()[dsNum]
        bLo, bHi = 0, nBkg
        if ds=="5A": bLo, bHi = 0, 79
        if ds=="5B": bLo, bHi = 80, 112
        if ds=="5C": bLo, bHi = 113, 121
        bkgRanges = bkg.getRanges(ds)

        # get psa cut runs and detector fitSlo efficiencies
        f = np.load('./data/lat-psa%dRunCut-ds%s.npz' % (pctTot,ds))
        psaRuns = f['arr_0'].item() # {ch: [runLo1, runHi1, runLo2, runHi2, ...]}
        fsD = dsi.getDBRecord("fitSlo_cpd_eff%d" % pctTot, False, calDB, pars)

        # get burst cut
        dsTmp = ds
        if ds=="5A": dsTmp=50
        if ds=="5B": dsTmp=51
        if ds=="5C": dsTmp=52
        iE = np.where(enrExc[:,0]==dsTmp)
        iN = np.where(natExc[:,0]==dsTmp)
        skipList = np.vstack((enrExc[iE], natExc[iN]))
        # print(skipList)

        # load ds_livetime output
        tl = TFile("./data/ds_%s_livetime.root" % str(ds))
        lt = tl.Get("dsTree")

        # 2. loop over modules
        mods = [1]
        if dsNum == 4: mods = [2]
        if dsNum == 5: mods = [1,2]
        for mod in mods:

            calKey = "ds%d_m%d" % (dsNum, mod)
            if ds == "5C": calKey = "ds5c"
            if calKey not in cal.GetKeys(dsNum):
                print("Error: Unknown cal key:",calKey)
                return

            print("Scanning DS-%s, m%d ..." % (ds, mod))

            chList = det.getGoodChanList(dsNum, mod)

            # save total efficiency for each channel in this DS
            totEff = {ch:np.zeros(len(xEff)) for ch in chList}
            trigEff = {ch:np.zeros(len(xEff)) for ch in chList}
            fSloEff = {ch:np.zeros(len(xEff)) for ch in chList}

            # 3. loop over bkgIdx
            for i, bIdx in enumerate(bkgRanges):


                # load bkg (trigger) and cal (PSA) cut coverage
                _,_, bkgCov, calCov = dsi.GetDBCuts(ds,bIdx,mod,"fr",calDB,pars,pctTot,False)

                rLo, rHi = bkgRanges[bIdx][0], bkgRanges[bIdx][-1]

                # get psa cut runs
                psaCutRuns = {ch:[] for ch in chList}
                for ch in chList:
                    if len(psaRuns[ch]) > 0:
                        for i in range(0,len(psaRuns[ch]),2):
                            psaCutRuns[ch].extend([r for r in range(psaRuns[ch][i],psaRuns[ch][i+1]+1) if rLo <= r <= rHi])

                # get burst cut runs
                burstCutRuns = {ch:False for ch in chList}
                for ch in chList:
                    cpd = det.getChanCPD(dsNum,ch)
                    iSkip = np.where((skipList == (dsTmp, int(cpd), bIdx)).all(axis=1))
                    if len(iSkip[0]) > 0:
                        burstCutRuns[ch] = True

                # 4. loop over sub-bIdx
                subRanges = bkg.GetSubRanges(ds, bIdx)
                if len(subRanges) == 0: subRanges.append((rLo, rHi))
                for sbIdx, (subLo, subHi) in enumerate(subRanges):

                    # load trigger efficiencies
                    key = "thresh_ds%d_bkg%d_sub%d" % (dsNum, bIdx, sbIdx)
                    thD = dsi.getDBRecord(key, False, calDB, pars)

                    # 5. loop over cIdx's in this sub-bIdx
                    cIdxLo, cIdxHi = cal.GetCalIdx(calKey, subLo), cal.GetCalIdx(calKey, subHi)
                    for i, cIdx in enumerate(range(cIdxLo, cIdxHi+1)):

                        # get the run coverage of this sub-sub-bIdx
                        if cIdxLo==cIdxHi:
                            covLo, covHi = subLo, subHi
                        else:
                            runList = bkg.getRunList(ds, bIdx)
                            subList = [r for r in runList if subLo <= r <= subHi and cal.GetCalIdx(calKey,r) == cIdx]
                            if len(subList)==0: continue
                            covLo, covHi = subList[0], subList[-1]

                        # calculate exposure for this sub-sub-bIdx
                        subExpo = {ch:0 for ch in chList}
                        n = lt.Draw("run:channel:livetime","run>=%d && run<=%d" % (covLo, covHi), 'goff')
                        ltRun, ltChan, ltLive = lt.GetV1(), lt.GetV2(), lt.GetV3()
                        for j in range(n):
                            ch = ltChan[j]
                            detID = det.getDetIDChan(dsNum,ch)
                            aMass = det.allActiveMasses[detID]
                            expo = ltLive[j]*aMass/86400/1000

                            # since we're splitting by module, ignore channels in the other module
                            # print(ds, mod, int(cpd[0]))
                            # exit()

                            if ch < 1000 and mod!=1: continue
                            if ch > 1000 and mod!=2: continue

                            if ltRun[j] in psaCutRuns[ch]:
                                continue
                            if burstCutRuns[ltChan[j]] is True:
                                continue
                            subExpo[ch] += expo

                            if detID > 100000:
                                enrExp[ds] += expo
                            else:
                                natExp[ds] += expo

                        # 6. loop over channels
                        for ch in chList:

                            goodThr = True if bkgCov[ch][sbIdx] else False
                            goodSlo = True if calCov[ch][0][i+1] else False
                            goodRise = True if calCov[ch][1][i+1] else False
                            if not (goodThr and goodSlo and goodRise):
                                continue

                            # finally, get trigger, fitSlo, and riseNoise efficiencies and scale by exposure

                            # trigger
                            mu, sig, isGood = thD[ch]
                            if isGood != 0:
                                print("error, bad threshold, ch",ch)
                                exit(1)
                            effThresh = mu + 3*sig
                            idx = np.where(xEff >= effThresh)
                            nPad = len(xEff) - len(xEff[idx])
                            tEff = wl.erFunc(xEff[idx],mu,sig,1)
                            tEff = np.pad(tEff, (nPad,0), 'constant')
                            tEff = np.multiply(tEff, subExpo[ch])

                            trigEff[ch] += tEff

                            # fitSlo & riseNoise
                            cpd = int(det.getChanCPD(dsNum,ch))
                            c, loc, scale, amp = fsD[cpd][3], fsD[cpd][4], fsD[cpd][5], fsD[cpd][2]
                            fEff = wl.weibull(xEff,c,loc,scale,amp)

                            riseEff = 0.995 # riseNoise is defined to be 99.5% efficient, no energy dependence
                            fEff = np.multiply(fEff, riseEff)

                            fSloEff[ch] += fEff

                            # total efficiency
                            if mode == "trig":
                                totEff[ch] += tEff
                            elif mode == "all":
                                totEff[ch] += np.multiply(tEff, fEff) # this is what we want
                            else:
                                print("Unknown mode! exiting ...")
                                exit()

            # plot individual efficiencies for a data set
            cmap = plt.cm.get_cmap('jet',len(chList)+1)
            cpdList = sorted([det.getChanCPD(dsNum,ch) for ch in chList])
            for iD, cpd in enumerate(cpdList):
                ch = det.getCPDChan(dsNum, str(cpd))
                if det.allDetIDs[str(cpd)] < 100000: continue
                if np.all(trigEff[ch]==0): continue

                idx = np.where(xEff < 2)
                print(cpd, np.sum(trigEff[ch][idx]))

                # plot 1 - trigger efficiency only
                # plt.plot(xEff, trigEff[ch], '-', c=cmap(iD), label="C%sP%sD%s" % (cpd[0],cpd[1],cpd[2])) # 1

                # plot 2 - slowness efficiency only
                plt.plot(xEff, fSloEff[ch], '-', c=cmap(iD), label="C%sP%sD%s" % (cpd[0],cpd[1],cpd[2])) # 2

                # plot 2a - normalized efficiency
                # plt.plot(xEff, fSloEff[ch]/np.amax(fSloEff[ch]), '-', c=cmap(iD), label="C%sP%sD%s" % (cpd[0],cpd[1],cpd[2])) # 2

                # plot 3 - total efficiency
                # plt.plot(xEff, totEff[ch], '-')

            # plt.xlim(0.5,6) # trigger limit
            plt.xlim(0.5, 30) # slowness limit

            plt.xlabel("Energy (keV)", ha='right', x=1)
            plt.ylabel("Enr. Exposure (kg-d)", ha='right', y=1)
            # plt.ylabel("Acceptance", ha='right', y=1)
            plt.legend(loc=4, ncol=3, fontsize=14)
            plt.tight_layout()
            # plt.show()
            # plt.savefig("./plots/lat-trigEff-DS3.pdf")
            # plt.savefig("./plots/lat-sloEff-DS3.pdf")
            plt.savefig("./plots/lat-sloEff-DS3-acc%d.pdf" % pctTot)
            exit()

        #     for ch in chList:
        #         cpd = det.getChanCPD(dsNum,ch)
        #         detEff[ds][cpd] = totEff[ch]
        #         # print(ch, cpd, detEff[ds][cpd][500:520], totEff[ch][500:520])
        #
        #     # plt.plot(xEff,detEff[ds]['164'])
        #     # plt.show()
        #     # exit()
        #
        # # ========= Done w/ 5 layer loop.  whew! =======
        #
        # # get total enr/nat efficiency for this DS
        # for cpd in detEff[ds]:
        #
        #     if det.allDetIDs[cpd] > 100000:
        #         totEnrEff[ds] += detEff[ds][cpd]
        #     else:
        #         totNatEff[ds] += detEff[ds][cpd]


def hi_mult_cal_spec():
    """ Adapted from sandbox/mult4.py
    sumSpec : {mHT: histos}
    hitData : [mHT, sumET, dt[mHT]]
    hitList : [hitE, chan, fSlo, rise, dtpc]  (same length as hitData)
    """
    # f1 = np.load("./data/mult4-sumE.npz")
    # f2 = np.load("./data/mult4-hitE.npz")
    f1 = np.load("./data/mult4-sumE-histats.npz")
    f2 = np.load("./data/mult4-hitE-histats.npz")
    runTime, x, sumSpec = f1['arr_0'], f1['arr_1'], f1['arr_2'].item()
    hitSpec, xHit, hitSpecLo = f1['arr_3'].item(), f1['arr_4'], f1['arr_5'].item()
    hitList, hitData, eCut = f2['arr_1'], f2['arr_2'], f2['arr_3']

    xLo, xHi, xpb = 0, 4000, 1

    # counts & rates for each multiplicity (use mHT)
    nHits = [sum(sumSpec[i]) for i in range(7)]
    nErr = [np.sqrt(nHits[i]) for i in range(7)]
    nPct = [100 / nErr[i] if nHits[i] > 0 else 0 for i in range(7)]
    rate = [nHits[i] / runTime for i in range(7)]
    rErr = [nErr[i] / runTime for i in range(7)]

    # sum spectrum
    fig = plt.figure()
    xLo, xHi, xpb = 0, 4000, 2
    cols = [0,'r','b','m','g','c','k']
    for mHT in range(1,5):
        pLabel = r'mHT=%d %.2f $\pm$ %.3f Hz' % (mHT,rate[mHT],rErr[mHT])
        plt.semilogy(x, sumSpec[mHT], ls='steps', lw=1.5, c=cols[mHT],label=pLabel)
    plt.xlabel("sumET (keV)", ha='right', x=1.)
    plt.ylabel("Counts / %.1f keV" % (xpb), ha='right', y=1.)
    plt.legend(fontsize=14)
    plt.tight_layout()
    plt.savefig("./plots/lat-sumSpec.pdf")

    # sum spectrum, events w/ 1 or more hit under 'eCut' keV
    plt.cla()
    n = len(hitList)
    for mHT in range(2,5):
        evts = [hitData[i][1] for i in range(n) if hitData[i][0] == mHT]
        plt.semilogy(*wl.GetHisto(evts,xLo,xHi,xpb), ls='steps', lw=1.5, c=cols[mHT], label='mHT=%d, eCut:%.0f keV' % (mHT, eCut))
    plt.xlabel("sumE (keV)", ha='right', x=1.)
    plt.ylabel("Counts / %.1f keV" % (xpb), ha='right', y=1.)
    plt.legend(fontsize=14)
    plt.tight_layout()
    plt.savefig("./plots/lat-selectSpec.pdf")

    # hit spectrum
    plt.cla()
    for mHT in range(1,5):
        plt.semilogy(x, hitSpec[mHT], ls='steps', lw=1.5, c=cols[mHT], label='mHT=%d' % mHT)
    plt.xlabel("hitE (keV)", ha='right', x=1.)
    plt.ylabel("Counts / %.1f keV" % (xpb), ha='right', y=1.)
    plt.legend(fontsize=14)
    plt.tight_layout()
    plt.savefig("./plots/lat-hitSpec.pdf")

    # hit spectrum, under 20 kev
    plt.cla()
    for mHT in range(1,5):
        plt.semilogy(xHit, hitSpecLo[mHT], ls='steps', lw=1.5, c=cols[mHT], label='mHT=%d' % mHT)
    plt.xlabel("hitE (keV)", ha='right', x=1.)
    plt.ylabel("Counts / %.1f keV" % (xpb), ha='right', y=1.)
    plt.legend(loc=1, fontsize=14)
    plt.tight_layout()
    plt.savefig("./plots/lat-hitSpecLow.pdf")


def m2s238_sum_peak():

    # f = np.load("./data/mult2-peaks.npz")
    f = np.load("./data/mult2-peaks-histats.npz") # full cal run
    runTime, pks, pkHist = f['arr_0'], f['arr_1'], f['arr_2'].item()

    # note: to access x and y histogram values:
    # x, y = pkHist[mH][iPk][0], pkHist[mH][iPk][1]

    def roughSigma(ene):
        """ from gpxFitter, just used to set initial guesses """
        p0, p1, p2 = 0.2, 0.02, 0.0003
        return np.sqrt(p0**2. + p1**2. * ene + p2**2. * ene**2.)


    def gaus(x, b, a, mu, sig):
        """ gaussian + flat bg """
        return b + a * np.exp(-(x-mu)**2. / (2. * sig**2.))

    from scipy.optimize import curve_fit

    pkE, mH, iPk = 238, 2, 0

    x, y = pkHist[mH][iPk][0], pkHist[mH][iPk][1]

    # xpb = 0.1
    # x = x + xpb/2.

    fig = plt.figure()

    plt.plot(x, y, c='b', ls='steps-mid', label="Calib, mHT==%d" % (mH))

    p0 = (np.mean(y[:5]), max(y), pkE, roughSigma(pkE))
    popt,_ = curve_fit(gaus, x, y, p0=p0)

    bpx = x[1] - x[0]
    bgRate = popt[0] * bpx
    mu, sig = popt[2], popt[3]
    idx = np.where((x > mu-3*sig) & (x < mu+3*sig))
    totCts = np.sum(y[idx])
    bgCts = bgRate * len(y[idx])
    pkCts = totCts - bgCts
    pbr = pkCts / bgCts
    print("%d  %.2f  %.2f  %.2f  %.2f  %d  %d  %d  P/B: %.2f" % (pkE, bpx, bgRate, mu, sig, totCts, bgCts, pkCts, pbr))

    sumLo, sumHi = mu-3*sig, mu+3*sig
    print("sumLo, sumHi = %.2f, %.2f" % (mu-3*sig, mu+3*sig))

    xVals = np.arange(236, 240, 0.01)

    plt.plot(xVals, gaus(xVals, *popt), 'r-', lw=4, alpha=0.7, label="%s = %.2f, 3%s = %.2f" % (r'$\mu$', popt[2], r'$\sigma$', 3*popt[3]))

    # 'fit.  mu %.2f  sig %.2f\nP/B %.2f' % (popt[2], popt[3], pbr))
    plt.axvline(sumLo, lw=3, alpha=0.7, c='g', label="%.2f - %.2f keV" % (sumLo, sumHi))
    plt.axvline(sumHi, lw=3, alpha=0.7, c='g')
    plt.plot(np.nan, np.nan, c='w', label="P/B = %.1f" % pbr)

    # plt.title("Peak-to-bkg ratio: %.3f" % pbr)
    plt.xlabel("sumET (keV)", ha='right', x=1.)
    plt.ylabel("Counts", ha='right', y=1.)
    plt.legend(loc=1)
    plt.xlim(236,242)
    plt.tight_layout()
    # plt.show()
    plt.savefig("./plots/lat-m%d-pk%d.pdf" % (mH,iPk))


def m2s238_hit_spec():
    """
    Adapted from sandbox/mult4.py::plotSpecTest.
    sumSpec : {mHT: histos}
    hitData : [mHT, sumET, dt[mHT]]
    hitList : [hitE, chan, fSlo]  (same length as hitData)
    """
    f1 = np.load("./data/mult4-sumE-histats.npz")
    f2 = np.load("./data/mult4-hitE-histats.npz")
    runTime, x, sumSpec = f1['arr_0'], f1['arr_1'], f1['arr_2'].item()
    hitSpec, xHit, hitSpecLo = f1['arr_3'].item(), f1['arr_4'], f1['arr_5'].item()
    hitList, hitData, eCut = f2['arr_1'], f2['arr_2'], f2['arr_3']

    # fitSlo results from tuneFitSlo.
    fsVals = {
        584: 102.5, 592: 75.5, 608: 73.5, 610: 76.5, 614: 94.5, 624: 69.5,
        626: 81.5, 628: 102.5, 632: 81.5, 640: 73.5, 648: 74.5, 658: 75.5,
        660: 127.5, 662: 84.5, 672: 80.5, 678: 82.5, 680: 86.5, 688: 77.5,
        690: 80.5, 694: 80.5
        }
    chList = fsVals.keys()

    mHT = 2

    hitE, chan, fSlo = [], [], []
    for i in range(len(hitData)):
        if hitData[i][0]==mHT and 237.28 < hitData[i][1] < 239.46:
        # if hitData[i][0]==mHT and 235 < hitData[i][1] < 240:
            hitE.extend(hitList[i][0])
            chan.extend(hitList[i][1])
            fSlo.extend(hitList[i][2])
    n = len(hitE)
    hitE = [hitE[i] for i in range(n) if chan[i] in fsVals.keys()]
    fSloShift = [fSlo[i]-fsVals[chan[i]] for i in range(n) if chan[i] in chList]

    hitESlow = [hitE[i] for i in range(len(hitE)) if fSloShift[i] > 30]
    hitEFast = [hitE[i] for i in range(len(hitE)) if fSloShift[i] < 30]

    xLo, xHi, xpb = 0, 250, 1
    x, hSlo = wl.GetHisto(hitESlow,xLo,xHi,xpb)
    x, hFast = wl.GetHisto(hitEFast,xLo,xHi,xpb)

    # load sim data
    f3 = np.load('./data/efficiency-corr250.npz')
    hTotSim, hSurfSim, xTotSim, simHists = f3['arr_0'], f3['arr_1'], f3['arr_2'], f3['arr_3'].item()
    hFracSim = np.divide(hSurfSim, hTotSim, dtype=float)

    idxS = np.where((xTotSim >= 1) & (xTotSim <= 238))
    slowFrac = 100 * np.sum(hSurfSim)/np.sum(hTotSim)

    fig = plt.figure()
    p1 = plt.subplot2grid((3,1), (0,0), rowspan=2)
    p2 = plt.subplot2grid((3,1), (2,0), sharex=p1)

    p1.plot(x, hFast/np.sum(hFast), ls='steps', c='b', lw=2., label='m2s238 Data')
    p1.plot(xTotSim, hTotSim/np.sum(hTotSim), ls='steps', c='r', lw=2., alpha=0.7, label='m2s238 Sim (Prelim.)')
    p1.plot(xTotSim, hSurfSim/np.sum(hTotSim), ls='steps', c='m', lw=2, label='Simulated Slow Hits')
    p1.axvline(123.3, c='g', lw=4, alpha=0.7, label=r"$E_C$: 123.3 keV")
    p1.plot(np.nan, np.nan, c='w', label="%.2f%% slow, 1-238 keV" % slowFrac)

    p1.set_ylabel("Counts (norm) / %.1f keV" % xpb, ha='right', y=1)
    p1.legend(loc=1, fontsize=12, bbox_to_anchor=(0., 0.4, 1, 0.2))

    p2.plot(xTotSim, 100*hFracSim, ls='steps', c='m', lw=2.)
    p2.axvline(1.0, c='b', lw=1, label="1.0 keV")

    p2.set_xlabel("Energy (keV)", ha='right', x=1)
    p2.set_ylabel("Pct. Slow")
    p2.legend(loc=1, fontsize=12)

    plt.tight_layout()
    fig.subplots_adjust(hspace=0.01)
    plt.setp(p1.get_xticklabels(), visible=False)

    # plt.show()
    plt.savefig("./plots/lat-238hits.pdf")


def save_slowness_data():
    """ Save the data for a plot of fitSlo vs energy, shifted and unshifted.
    Saving s/t I can use the thesis plot format on my mac.
    """
    from ROOT import TChain
    import tinydb as db

    ds, cIdx, calKey = 1, 1, "ds1_m1"

    # load the fitSlo values for this cIdx
    shiftVals = {}
    calDB = db.TinyDB('%s/calDB-v2.json' % (dsi.latSWDir))
    pars = db.Query()
    fsD = dsi.getDBRecord("fitSlo_%s_idx%d_m2s238" % (calKey, cIdx), False, calDB, pars)
    chList = det.getGoodChanList(ds)
    for ch in chList:
        if ch not in fsD.keys():
            print("Error, channel %d not found" % ch)
            exit()
        # "fitSlo_[calKey]_idx[ci]_m2s238" : {ch : [fsCut, fs200] for ch in chList}}
        # NOTE: fsCut is the 90% value, already shifted back for this calibration index
        #       fs200 is the mean fitSlo value for this channel.
        # print(ch, fsD[ch][0], fsD[ch][1])
        if fsD[ch] is not None and fsD[ch][0] > 0:
            shiftVals[ch] = fsD[ch][1]
    cutChanList = sorted(shiftVals.keys())
    # for ch in cutChanList:
        # print(ch, shiftVals[ch])

    # load the cal runs
    fList = []
    cRuns = cal.GetCalList(calKey, cIdx)
    for run in cRuns:
        latList = dsi.getSplitList("%s/latSkimDS%d_run%d*" % (dsi.calLatDir, ds, run), run)
        tmpList = [f for idx, f in sorted(latList.items())]
        fList.extend(tmpList)


    tt = TChain("skimTree")
    for f in fList: tt.Add(f)

    n = tt.Draw("trapENFCal:fitSlo:channel","","goff")
    hitE, fSlo, chan = tt.GetV1(), tt.GetV2(), tt.GetV3()
    hitE = [hitE[i] for i in range(n)]
    fSlo = [fSlo[i] for i in range(n)]
    chan = [chan[i] for i in range(n)]

    # unshifted values
    hitE1 = [hitE[i] for i in range(n) if chan[i] in cutChanList]
    fSlo1 = [fSlo[i] for i in range(n) if chan[i] in cutChanList]

    # shifted values
    hitE2 = hitE1
    fSlo2 = [fSlo[i] - shiftVals[chan[i]] for i in range(n) if chan[i] in cutChanList]

    np.savez("./data/lat-slowness.npz", hitE1, fSlo1, fSlo2)


def slowness_vs_energy():

    from matplotlib import colors
    # myNorm = colors.PowerNorm(gamma=0.2) # < used this one before
    # myNorm = colors.PowerNorm(gamma=1)
    # myNorm = colors.PowerNorm(gamma=0.1)
    myNorm = colors.LogNorm()

    f = np.load("./data/lat-slowness.npz")
    hitE1, fSlo1, fSlo2 = f['arr_0'], f['arr_1'], f['arr_2']

    fig = plt.figure(figsize=(10,5))
    p1 = plt.subplot(121)
    p2 = plt.subplot(122)

    xLo, xHi, xpb = 0, 250, 1
    nbx = int((xHi-xLo)/xpb)

    yLo1, yHi1, ypb1 = 0, 200, 1
    nby1 = int((yHi1-yLo1)/ypb1)

    yLo2, yHi2, ypb2 = -75, 125, 1
    nby2 = int((yHi2-yLo2)/ypb2)

    # h1 = p1.hist2d(hitE1, fSlo1, bins=[nbx, nby1], range=[[xLo,xHi],[yLo1,yHi1]])
    # h2 = p2.hist2d(hitE1, fSlo2, bins=[nbx, nby2], range=[[xLo,xHi],[yLo2,yHi2]])
    # h1max = np.amax(h1[0])
    # h2max = np.amax(h2[0])
    # plt.cla()
    h1max = 2766
    h2max = 7822

    h1 = p1.hist2d(hitE1, fSlo1, bins=[nbx, nby1], range=[[xLo,xHi],[yLo1,yHi1]], vmin=2, vmax=h1max, cmap='jet', norm=myNorm)
    p1.set_xlabel("Energy (keV)", ha='right', x=1)
    p1.set_ylabel("fitSlo", ha='right', y=1)
    p1.annotate('Unshifted', xy=(210, 270), xycoords='axes points', size=14, ha='right', va='center', bbox=dict(boxstyle='round', fc='w', alpha=0.9))
    cb1 = fig.colorbar(h1[3], ax=p1)
    cb1.ax.minorticks_off()

    h2 = p2.hist2d(hitE1, fSlo2, bins=[nbx, nby2], range=[[xLo,xHi],[yLo2,yHi2]], vmin=2, vmax=h2max, cmap='jet', norm=myNorm)
    p2.set_xlabel("Energy (keV)", ha='right', x=1)
    p2.annotate('Shifted', xy=(210, 270), xycoords='axes points', size=14, ha='right', va='center', bbox=dict(boxstyle='round', fc='w', alpha=0.9))
    # p2.set_ylabel("fitSlo (shifted)", ha='right', y=1)
    cb2 = fig.colorbar(h2[3], ax=p2)
    cb2.ax.minorticks_off()


    print("h1 max:",np.amax(h1[0]))
    print("h2 max:",np.amax(h2[0]))

    plt.tight_layout()
    # plt.show()
    plt.savefig("./plots/lat-fitSlo-dist.pdf")


def fitSlo_stability():
    """ Adapted from LAT/sandbox/slo-cut.py::plotEff
    That function made seven plots:
    1. hit spectrum, all channels, 0-250
    2. bar plot, hits in all channels
    3. 2d hits vs channels, 0-20 keV
    4. typical fitSlo values
    5. m2s238 slowness
    5. difference between m2s238 and pk238 90pct cut values
    6. stability of fitSlo vs run number (calIdx)
    """
    import os

    # arrays to plot m2s238 data
    effHitE = []  # [hitE1, hitE2 , ...] (remove sub-list of input format)
    effChan = []  # [chan1, chan2 , ...]
    effSlo = []   # [fSlo1, fSlo2, ...]
    effRise = []  # [rise1, rise2, ...]
    effRun = []   # [run1, run1, ...]

    sloSpec = [] # array of fitSlo histo dicts (i should have used pandas probably)

    # load efficiency files
    fList = []
    for ds in [4]:
        # print("Loading DS-%d" % ds)
        for key in cal.GetKeys(ds):
            mod = -1
            if "m1" in key: mod = 1
            if "m2" in key: mod = 2
            for cIdx in range(cal.GetIdxs(key)):
                eFile = "%s/eff_%s_c%d.npz" % (dsi.effDir, key, cIdx)
                if os.path.isfile(eFile):
                    fList.append([ds,cIdx,mod,eFile])
                else:
                    print("File not found:",eFile)
                    continue
    for ds,ci,mod,ef in fList:
        # print(ds,ci,mod,ef)
        f = np.load(ef)
        evtIdx = f['arr_0']          # m2s238 event [[run,iE] , ...]
        evtSumET = f['arr_1']        # m2s238 event [sumET , ...]
        evtHitE = f['arr_2']         # m2s238 event [[hitE1, hitE2] , ...]
        evtChans = f['arr_3']        # m2s238 event [[chan1, chan2] , ...]
        thrCal = f['arr_4'].item()   # {ch : [run,thrM,thrS,thrK] for ch in goodList(ds)}
        thrFinal = f['arr_5'].item() # {ch : [thrAvg, thrDev] for ch in goodList(ds)}
        evtCtr = f['arr_6']          # num m2s238 evts
        totCtr = f['arr_7']          # num total evts
        runTime = f['arr_8']         # cal run time
        fSloSpec = f['arr_9'].item() # fitSlo histos (all hits) {ch:[h10, h200, h238] for ch in chList}
        fSloX = f['arr_10']          # xVals for fitSlo histos
        evtSlo = f['arr_11']         # m2s238 event [[fSlo1, fSlo2], ...]
        evtRise = f['arr_12']        # m2s238 event [[rise1, rise2], ...]

        sloSpec.append(fSloSpec)

        # remove the hit pair
        for i in range(len(evtHitE)):
            effHitE.extend(evtHitE[i])
            effChan.extend(evtChans[i])
            effSlo.extend(evtSlo[i])
            effRise.extend(evtRise[i])
            effRun.extend([evtIdx[i][0],evtIdx[i][0]])

    effHitE = np.asarray(effHitE)
    effChan = np.asarray(effChan)
    effSlo = np.asarray(effSlo)
    effRun = np.asarray(effRun)

    chList = det.getGoodChanList(ds)
    cpdList = sorted([det.getChanCPD(ds,ch) for ch in chList])

    # plot 6 -- stability of DS4

    fig = plt.figure()
    cmap = plt.cm.get_cmap('jet', len(chList)+1)

    for i, cpd in enumerate(cpdList):
        ch = det.getCPDChan(ds, cpd)
        type = "e" if det.allDetIDs[cpd] > 100000 else "n"
        # print(i, cpd, ch)
        plt.plot(np.nan, np.nan, ".-", c=cmap(i), label="C%sP%sD%s (%s)" % (cpd[0],cpd[1],cpd[2], type))

        fs200, x200, fsm2s238 = [], [], []
        for ci in range(len(sloSpec)):

            # only save the value if we have a nonzero number of counts
            spec = sloSpec[ci][ch][1]
            nCts = np.sum(spec)
            if nCts < 2: continue
            # print(ds,ch,ci,nCts)

            # get the width
            max, avg, std, pct, wid = wl.getHistInfo(fSloX, sloSpec[ci][ch][1])

            # TODO: smarter way to get the width
            # like a FWHM.  find the max, then find the point of 50% reduction on either side

            fs200.append(fSloX[np.argmax(sloSpec[ci][ch][1])])
            x200.append(ci)

            # get m2s238 events from this calIdx and find the typical value
            idx = np.where(effChan==ch)
            tmpS = effSlo[idx]
            tmpC = effChan[idx]
            tmpR = effRun[idx]
            thisFS = []
            for j in range(len(tmpR)):
                key = "ds%d_m1" % ds if ch < 1000 else "ds%d_m2" % ds
                if ci == cal.GetCalIdx(key,tmpR[j]):
                    thisFS.append(tmpS[j])
            nEff = len(thisFS)

            yLo, yHi, ypb = -50, 400, 1
            x, hSlo = wl.GetHisto(thisFS, yLo, yHi, ypb)
            maxEff = np.nan if len(thisFS)==0 else x[np.argmax(hSlo)]

            # NOTE: the diff is NEVER more than 1.
            # print("%d  %-3d  nTot %-8d  nEff %-5d  wid %-4.0f  fs200 %-4.0f  fsEff %-4.0f  diff %.0f" % (ch, ci, nCts, nEff, wid, fs200[-1], maxEff, fs200[-1]-maxEff))

            fsm2s238.append(maxEff)


        # plot the raw value (stability)
        plt.plot(x200, fs200, ".-", c=cmap(i))
        # plt.axhline(np.mean(fs200), c=cmap(i), linewidth=0.5, label="ch%d: %.2f")
        plt.ylim(0,150)

        # plot the difference from the average (deviation)
        # fAvg = np.mean(fs200)
        # fDev = [(f-fAvg) for f in fs200]
        # p2.plot(x200, fDev, ".", c=cmap(i), label="ch%d  fAvg %.0f" % (ch, fAvg))

        # plot the difference between the raw value and the m2s238 value
        # man, i shoulda just added the calIdx of the m2s238 hits

    plt.xlabel("DS4, calIdx", ha='right', x=1)
    plt.ylabel("fitSlo", ha='right', y=1)
    plt.xticks(np.arange(0,len(sloSpec),1))
    plt.legend(loc=3, ncol=3, fontsize=14)
    # if ds!=5: p1.legend(ncol=3)
    # else: p1.legend(ncol=6, fontsize=8)
    # p2.set_ylabel("fitSlo Deviation from avg", ha='right', y=1)
    plt.tight_layout()
    # plt.show()
    plt.savefig("./plots/lat-stability-ds%d.pdf" % (ds))


def fitSlo_distribution():
    """ Adapted from mult4.py::plotFitSloHist.
    hitData : [mHT, sumET, dt[mHT]]
    hitList : [hitE, chan, fSlo, rise, dtpc]  (same length as hitData)
    """
    # fitSlo results from tuneFitSlo.
    fsVals = {
        584: 102.5, 592: 75.5, 608: 73.5, 610: 76.5, 614: 94.5, 624: 69.5,
        626: 81.5, 628: 102.5, 632: 81.5, 640: 73.5, 648: 74.5, 658: 75.5,
        660: 127.5, 662: 84.5, 672: 80.5, 678: 82.5, 680: 86.5, 688: 77.5,
        690: 80.5, 694: 80.5
        }

    chList = list(fsVals.keys())

    # peak fit results from mult2.  could retune, but it shouldn't matter much
    mHT = 2
    sumPks = [
        (2,238,237.28,239.46), (2,583,581.26,584.46), (2,2615,2610.57,2618.01),
        (3,238,237.13,239.43), (3,583,581.04,584.36), (3,2615,2610.10,2617.92)
        ]

    # f1 = np.load("./data/mult4-hitE.npz")
    f1 = np.load("./data/mult4-hitE-histats.npz")
    runTime, hitList, hitData, eCut = f1['arr_0'], f1['arr_1'], f1['arr_2'], f1['arr_3']

    hitE, chan, fSlo = [], [], []
    for i in range(len(hitData)):
        if hitData[i][0]==mHT and 237.28 < hitData[i][1] < 239.46:
            hitE.extend(hitList[i][0])
            chan.extend(hitList[i][1])
            fSlo.extend(hitList[i][2])
    n = len(hitE)
    hitE = [hitE[i] for i in range(n) if chan[i] in fsVals.keys()]
    fSloShift = [fSlo[i]-fsVals[chan[i]] for i in range(n) if chan[i] in chList]
    print("peak evts:",n)

    cLo, cHi = 0, 230

    hitE2, chan2, fSlo2 = [], [], []
    for i in range(len(hitData)):
        if hitData[i][0]==mHT and cLo < hitData[i][1] < cHi:
            hitE2.extend(hitList[i][0])
            chan2.extend(hitList[i][1])
            fSlo2.extend(hitList[i][2])
    n = len(hitE2)
    hitE2 = [hitE2[i] for i in range(n) if chan2[i] in fsVals.keys()]
    fSloShift2 = [fSlo2[i]-fsVals[chan2[i]] for i in range(n) if chan2[i] in chList]
    print("cont evts:",n)

    fig = plt.figure()

    xLo, xHi, xpb = 0, 250, 1
    nbx = int((xHi-xLo)/xpb)
    yLo, yHi, ypb = -50, 400, 1
    nby = int((yHi-yLo)/ypb)

    # # ===== 1.  2d fitSlo vs energy, m2s238 hits, shifted. ======
    #
    # hitE2, chan2, fSlo2 = [], [], []
    # for i in range(len(hitData)):
    #     if hitData[i][0]==mHT and cLo < hitData[i][1] < cHi:
    #         hitE2.extend(hitList[i][0])
    #         chan2.extend(hitList[i][1])
    #         fSlo2.extend(hitList[i][2])
    # n = len(hitE2)
    # hitE2 = [hitE2[i] for i in range(n) if chan2[i] in fsVals.keys()]
    # fSloShift = [fSlo2[i]-fsVals[chan2[i]] for i in range(n) if chan2[i] in chList]
    # print("cont evts:",n)
    #
    # plt.hist2d(hitE2, fSloShift, bins=[nbx, nby], range=[[xLo,xHi],[yLo,yHi]], norm=LogNorm(), cmap='jet', label='m=2, hits 0-230 keV')
    # plt.colorbar()
    # plt.xlabel("Energy (keV)", ha='right', x=1.)
    # plt.ylabel("fitSlo", ha='right', y=1.)
    # plt.tight_layout()
    # plt.savefig("./plots/mult4-fitSlo-shift2-hist.png")


    # ===== 2.

    # cb.remove()
    plt.cla()
    xLo, xHi, xpb = -50, 300, 1

    x, y238 = wl.GetHisto(fSloShift,xLo,xHi,xpb)
    x, yCon = wl.GetHisto(fSloShift2,xLo,xHi,xpb)

    # integral238
    tot238 = np.sum(y238)
    int238, x238 = 0, 0
    for i in range(len(y238)):
        int238 += y238[i]
        if int238/tot238 > 0.90:
            x238 = x[i]
            break

    # integralCon
    totCon = np.sum(yCon)
    intCon, xCon = 0, 0
    for i in range(len(yCon)):
        intCon += yCon[i]
        if intCon/totCon > 0.95:
            xCon = x[i]
            break

    plt.semilogy(x, yCon/np.sum(yCon), 'r', ls='steps', label='All hits, %d-%d keV' % (cLo, cHi))
    plt.semilogy(x, y238/np.sum(y238), 'b', ls='steps', label='m2s238 Hits')

    # plt.axvline(x238, c='g', label='m2s238 90%, all dets')

    plt.xlabel("fitSlo (Shifted)", ha='right', x=1.)
    plt.ylabel("Counts (Normalized)", ha='right', y=1.)

    plt.legend(loc=1)
    plt.tight_layout()
    # plt.show()
    plt.savefig("./plots/lat-fitSlo-hist.pdf")


def fitSlo_det_efficiencies():
    """ Load the efficiency data from LAT2 directly for faster plotting.
    This is where we do the final efficiency fit, apparently.
    """
    from statsmodels.stats import proportion
    from scipy.optimize import curve_fit

    # must match lat2
    # pctTot = 90
    pctTot = 95
    xPassLo, xPassHi, xpbPass = 0, 50, 1      # "low energy" region

    print("Using pctTot ==",pctTot)

    # weibull fit constraints: energy, (c, loc, scale, amp)
    eFitHi = 30
    # fitBnd = ((0,-15,0,0),(np.inf,np.inf,np.inf,1.)) # this is the original
    fitBnd = ((1,-20,0,0.5),(np.inf,np.inf,np.inf, 0.99)) # eFitHi=30 and these works!

    # load efficiency data
    f = np.load('./data/lat2-eff-data-%d.npz' % pctTot)
    effData = f['arr_0'].item()

    fig = plt.figure(3) # efficiency plot
    p1 = plt.subplot2grid((3,1), (0,0), rowspan=2)
    p2 = plt.subplot2grid((3,1), (2,0), sharex=p1)

    print("CPD   amp     c       loc      scale   e1keV  nBin   maxR    chi2")
    detList = det.allDets
    for i, cpd in enumerate(detList):
    # for i, cpd in enumerate(['122', '114', '273', '133']): # these have convergence issues
    # for i, cpd in enumerate(['133']):

        if cpd not in effData.keys():
            # print(cpd)
            # print("%s & & & & & & & & & \\\\" % cpd)
            continue

        xEff, sloEff, ci_low, ci_upp = effData[cpd][0], effData[cpd][1], effData[cpd][2], effData[cpd][3]
        hPass, hFail, hTot, xELow = effData[cpd][4], effData[cpd][5], effData[cpd][6], effData[cpd][7]

        # get the average number of counts per bin under 10 kev, both passing and failing
        # nBin = np.sum(hTot[np.where(xELow < 10)])/(10/xpbPass)
        nBin = np.sum(hPass[np.where(xELow < 10)])/(10/xpbPass)

        # efficiency for low-E region (xPassLo, xPassHi)

        # start by only looking at points where we have >0 hits passing
        idxP = np.where(hPass > 0)
        sloEff = hPass[idxP] / hTot[idxP]
        xEff = xELow[idxP]

        # calculate confidence intervals for each point (error bars)
        ci_low, ci_upp = proportion.proportion_confint(hPass[idxP], hTot[idxP], alpha=0.1, method='beta')

        # limit the energy range
        idxE = np.where((xEff < xPassHi) & (xEff > 1))
        xEff, sloEff, ci_low, ci_upp = xEff[idxE], sloEff[idxE], ci_low[idxE], ci_upp[idxE]

        # ** this is essential, otherwise the blue dots show up on the right side of
        # where the bin center would be, and throw off the fit by half a bin width
        xEff -= xpbPass/2.

        # limit the fit energy range
        idxF = np.where(xEff <= eFitHi)

        # run the fit
        popt, pcov = curve_fit(wl.weibull, xEff[idxF], sloEff[idxF], bounds=fitBnd)

        # save pars and errors
        perr = np.sqrt(np.diag(pcov))
        c, loc, sc, amp = popt
        cE, locE, scE, ampE = perr
        eff1 = wl.weibull(1.,*popt)

        from scipy.stats import chisquare
        # "This test is invalid when the observed or expected frequencies in each category are too small. A typical rule is that all of the observed and expected frequencies should be at least 5."
        # wenqin: the bin errors are binomial.  see https://root.cern.ch/doc/master/classTEfficiency.html
        chi2, p = chisquare(sloEff, wl.weibull(xEff,*popt))

        # get residual
        hResidSig = []
        n = len(sloEff)
        for i in range(n):
            diff = wl.weibull(xEff[i], *popt) - sloEff[i]
            sig = ci_upp[i] if diff > 0 else ci_low[i]
            if sig==0:
                # print("zero at",xEff[i],"kev")
                hResidSig.append(0)
                continue
            hResidSig.append(diff/sig)
        hResidSig = np.asarray(hResidSig)

        # find abs. max of residual
        maxR = np.max(np.fabs(hResidSig))

        # print results
        # print("%s  %-4.3f  %-4.1f  %-7.2f  %-5.2f  %-3.2f  %d  %.3f  %.3f" % (cpd, amp, c, loc, sc, eff1, nBin, maxR, chi2))

        # print latexable results
        print("%s & %-4.3f & %-4.1f & %-7.2f & %-5.2f & %-3.2f & %-4d & %-5.3f & %-5.3f \\\\" % (cpd, amp, c, loc, sc, eff1, nBin, maxR, chi2))
        continue

        # print ugly one with errors
        # print("%s  a %.3f (%.3f)  c %.1f (%.1f)  loc %.2f (%.2f)  sc %.2f (%.2f)  eff1 %.2f  nBin %d" % (cpd, amp, ampE, c, cE, loc, locE, sc, scE, eff1, nBin))

        # === make the efficiency plot, with sigma residual ===

        # plot efficiency
        p1.cla()
        p1.plot(xEff, sloEff, '.b', ms=10., label='C%sP%sD%s, nBin %.1f' % (cpd[0],cpd[1],cpd[2],nBin))
        p1.errorbar(xEff, sloEff, yerr=[sloEff - ci_low, ci_upp - sloEff], color='k', linewidth=0.8, fmt='none')

        xFunc = np.arange(xPassLo, xPassHi, 0.1)
        p1.plot(xFunc, wl.weibull(xFunc, *popt), 'g-', label=r'Weibull CDF')
        p1.axvline(1.,color='b', lw=1., label='1.0 keV efficiency: %.2f' % wl.weibull(1.,*popt))

        p1.set_xlim(xPassLo, xPassHi)
        p1.set_ylim(0,1)
        # p1.set_xlabel("hitE (keV)", ha='right', x=1)
        p1.set_ylabel("Efficiency", ha='right', y=1)
        p1.yaxis.set_label_coords(-0.095, 1.)
        p1.legend(loc=4)

        # plot residual
        p2.cla()
        p2.plot(xEff, hResidSig, ".g")
        p2.annotate('Residual, Fit - Data', xy=(470, 80), xycoords='axes points', size=14, ha='right', va='center', bbox=dict(boxstyle='round', fc='w', alpha=0.75, edgecolor='gray'))
        p2.axvline(1.,color='b', lw=1.)

        p2.set_xlim(xPassLo, xPassHi)
        yLo = -0.4 if np.min(hResidSig) > -0.4 else np.min(hResidSig) * 1.1
        yHi = 0.4 if np.max(hResidSig) < 0.4 else np.max(hResidSig) * 1.1
        p2.set_ylim(yLo, yHi)

        p2.set_ylabel("Residual (Sigma)")
        p2.set_xlabel("Energy (keV)", ha='right', x=1)
        p2.set_xticks(np.arange(0,51,5))

        plt.tight_layout()
        fig.subplots_adjust(hspace=0.01)
        plt.setp(p1.get_xticklabels(), visible=False)

        # plt.show()
        # exit()
        plt.savefig("./plots/lat2-eff%d-%s.pdf" % (pctTot,cpd))


def fitSlo_tot_efficiencies():
    """ Combine all enriched/natural detectors and plot an overall efficiency.
    This is more for curiosity
    """
    from statsmodels.stats import proportion
    from scipy.optimize import curve_fit

    # very important parameter
    # pctTot = 90
    pctTot = 95

    print("Using pctTot ==",pctTot)

    # overall hit and efficiency plots (low-E region)
    xPassLo, xPassHi, xpbPass = 0, 50, 1      # "low energy" region
    xTot, hPassEnr = wl.GetHisto([], xPassLo, xPassHi, xpbPass, shift=False)
    xTot, hFailEnr = wl.GetHisto([], xPassLo, xPassHi, xpbPass, shift=False)
    xTot, hTotEnr = wl.GetHisto([], xPassLo, xPassHi, xpbPass, shift=False)
    xTot, hPassNat = wl.GetHisto([], xPassLo, xPassHi, xpbPass, shift=False)
    xTot, hFailNat = wl.GetHisto([], xPassLo, xPassHi, xpbPass, shift=False)
    xTot, hTotNat = wl.GetHisto([], xPassLo, xPassHi, xpbPass, shift=False)

    # weibull fit constraints: energy, (c, loc, scale, amp)
    eFitHi = 30
    fitBnd = ((1,-20,0,0.5),(np.inf,np.inf,np.inf, 0.99)) # eFitHi=30 and these works!

    # load efficiency data
    f = np.load('./data/lat2-eff-data-%d.npz' % pctTot)
    effData = f['arr_0'].item()
    detList = det.allDets
    for i, cpd in enumerate(detList):
        if cpd not in effData.keys():
            continue
        xEff, sloEff, ci_low, ci_upp = effData[cpd][0], effData[cpd][1], effData[cpd][2], effData[cpd][3]
        hPass, hFail, hTot, xELow = effData[cpd][4], effData[cpd][5], effData[cpd][6], effData[cpd][7]

        if det.isEnr(cpd):
            hTotEnr = np.add(hTotEnr, hTot)
            hPassEnr = np.add(hPassEnr, hPass)
            hFailEnr = np.add(hFailEnr, hFail)
        else:
            hTotNat = np.add(hTotNat, hTot)
            hPassNat = np.add(hPassNat, hPass)
            hFailNat = np.add(hFailNat, hFail)

    # === calculate enr/nat efficiency, see above for a more verbose explanation  ===

    fig = plt.figure()

    # enriched efficiency
    nBinEnr = np.sum(hPassEnr[np.where(xELow < 10)])/(10/xpbPass)
    idxP = np.where(hPassEnr > 0)
    sloEffEnr = hPassEnr[idxP] / hTotEnr[idxP]
    xEffEnr = xELow[idxP]
    ciLowEnr, ciUppEnr = proportion.proportion_confint(hPassEnr[idxP], hTotEnr[idxP], alpha=0.1, method='beta')
    idxE = np.where((xEffEnr < xPassHi) & (xEffEnr > 1))
    xEffEnr, sloEffEnr, ciLowEnr, ciUppEnr = xEffEnr[idxE], sloEffEnr[idxE], ciLowEnr[idxE], ciUppEnr[idxE]
    xEffEnr -= xpbPass/2.
    idxF = np.where(xEffEnr <= eFitHi)
    poptEnr,_ = curve_fit(wl.weibull, xEffEnr[idxF], sloEffEnr[idxF], bounds=fitBnd)

    # natural efficiency
    nBinNat = np.sum(hPassNat[np.where(xELow < 10)])/(10/xpbPass)
    idxP = np.where(hPassNat > 0)
    sloEffNat = hPassNat[idxP] / hTotNat[idxP]
    xEffNat = xELow[idxP]
    ciLowNat, ciUppNat = proportion.proportion_confint(hPassNat[idxP], hTotNat[idxP], alpha=0.1, method='beta')
    idxE = np.where((xEffNat < xPassHi) & (xEffNat > 1))
    xEffNat, sloEffNat, ciLowNat, ciUppNat = xEffNat[idxE], sloEffNat[idxE], ciLowNat[idxE], ciUppNat[idxE]
    xEffNat -= xpbPass/2.
    idxF = np.where(xEffNat <= eFitHi)
    poptNat,_ = curve_fit(wl.weibull, xEffNat[idxF], sloEffNat[idxF], bounds=fitBnd)

    # plot overall enr/nat hit spectrum
    plt.plot(xTot, hTotEnr, ls='steps', c='k', label="Enr, Total Hits")
    plt.plot(xTot, hPassEnr, ls='steps', c='b', label="Enr, Pass")
    plt.plot(xTot, hFailEnr, ls='steps', c='r', label="Enr, Fail")
    plt.plot(xTot, hTotNat, ls='steps', c='m', label="Nat, Total Hits")
    plt.plot(xTot, hPassNat, ls='steps', c='g', label="Nat, Pass")
    plt.plot(xTot, hFailNat, ls='steps', c='orange', label="Nat, Fail")

    plt.axvline(1., c='g', lw=1, label='1 kev')
    plt.xlabel("Energy (keV)", ha='right', x=1)
    plt.ylabel("Counts/%.1f keV" % xpbPass, ha='right', y=1)
    plt.xlim(xPassLo, xPassHi)
    plt.legend(loc=1, bbox_to_anchor=(0., 0.5, 1, 0.2))
    plt.tight_layout()
    # plt.show()
    plt.savefig("./plots/lat2-eff%d-totHits.pdf" % pctTot)

    # efficiency plot, with sigma residual

    # renaming key:
    # xEff, sloEff, nBin, ci_low, ci_upp, popt
    # xEffEnr, sloEffEnr, nBinEnr, ciLowEnr, ciUppEnr, poptEnr
    # xEffNat, sloEffNat, nBinNat, ciLowNat, ciUppNat, poptNat

    plt.close()
    # fig = plt.figure(figsize=(8,6))
    # p1 = plt.subplot2grid((3,1), (0,0), rowspan=2)
    # p2 = plt.subplot2grid((3,1), (2,0), sharex=p1)
    fig = plt.figure()
    p1 = plt.subplot(111)
    xFunc = np.arange(xPassLo, xPassHi, 0.1)
    p1.axvline(1, c='k', lw=1)
    # p2.axvline(1, c='k', lw=1)

    p1.plot(xEffEnr, sloEffEnr, '.b', ms=10., label='Enriched Detectors, nBin %.1f' % (nBinEnr))
    p1.errorbar(xEffEnr, sloEffEnr, yerr=[sloEffEnr - ciLowEnr, ciUppEnr - sloEffEnr], color='k', linewidth=0.8, fmt='none')
    p1.plot(xFunc, wl.weibull(xFunc, *poptEnr), 'b-', lw=2, alpha=0.8)
    p1.axvline(np.nan, np.nan, c="w", label='1.0 keV enr eff: %.2f' % wl.weibull(1.,*poptEnr))

    p1.plot(xEffNat, sloEffNat, '.m', ms=10., label='Natural Detectors, nBin %.1f' % (nBinNat))
    p1.errorbar(xEffNat, sloEffNat, yerr=[sloEffNat - ciLowNat, ciUppNat - sloEffNat], color='k', linewidth=0.8, fmt='none')
    p1.plot(xFunc, wl.weibull(xFunc, *poptNat), 'm-', lw=2, alpha=0.8)
    p1.axvline(np.nan, np.nan, c="w", label='1.0 keV nat eff: %.2f' % wl.weibull(1.,*poptNat))

    p1.set_xlim(xPassLo, xPassHi)
    p1.set_ylim(0.2,1)
    p1.set_xlabel("hitE (keV)", ha='right', x=1)
    p1.set_ylabel("Efficiency", ha='right', y=1)
    p1.yaxis.set_label_coords(-0.095, 1.)
    p1.legend(loc=4)

    plt.tight_layout()
    # plt.show()
    plt.savefig("./plots/lat2-eff%d.pdf" % (pctTot))

    # p2.plot(xEff, hResidSig, ".g")
    # p2.annotate('Residual, Fit - Data', xy=(470, 80), xycoords='axes points', size=14, ha='right', va='center', bbox=dict(boxstyle='round', fc='w', alpha=0.75, edgecolor='gray'))
    # p2.axvline(1.,color='b', lw=1.)
    # p2.set_xlim(xPassLo, xPassHi)
    # yLo = -0.4 if np.min(hResidSig) > -0.4 else np.min(hResidSig) * 1.1
    # yHi = 0.4 if np.max(hResidSig) < 0.4 else np.max(hResidSig) * 1.1
    # p2.set_ylim(yLo, yHi)
    # p2.set_ylabel("Residual (Sigma)")
    # p2.set_xlabel("Energy (keV)", ha='right', x=1)
    # p2.set_xticks(np.arange(0,51,5))
    # plt.tight_layout()
    # fig.subplots_adjust(hspace=0.01)
    # plt.setp(p1.get_xticklabels(), visible=False)


def fitSlo_exposure_weighted_eff():

    f = np.load(dsi.latSWDir+'/data/lat-expo-efficiency-all.npz')

    xEff = f['arr_0']
    eMin, eMax, nBins = 0, 50, len(xEff)
    totEnrEff = f['arr_1'].tolist()
    totNatEff = f['arr_2'].tolist()
    enrExp = f['arr_3'].tolist()
    natExp = f['arr_4'].tolist()

    cmap = plt.cm.get_cmap('brg',len(totEnrEff)+1)

    for i, ds in enumerate(totEnrEff):
        plt.plot(xEff, 0.9 * totEnrEff[ds]/np.max(totEnrEff[ds]), c=cmap(i), label="DS-%s" % str(ds))

    plt.xlim(0,15)
    plt.xlabel("Energy (keV)", ha='right', x=1)
    plt.ylabel("Enr. Acceptance", ha='right', y=1)
    plt.yticks(np.arange(0,1.1,0.1))
    plt.legend(loc=4)
    plt.tight_layout()
    plt.show()


def get_ext_pulser_data():
    """ Adapted from ext2.py::getEff
    Just gets data and save into a npz file.
    """
    from ROOT import TChain, GATDataSet
    import glob

    # this is the output
    extData = {} # {run: [pIdx, runTime, extChan, hitE, fSlo]}

    for pIdx in [19,20,21]:
    # for pIdx in [19]:

        extPulserInfo = cal.GetSpecialList()["extPulserInfo"]
        attList = extPulserInfo[pIdx][0] # unused
        extChan = extPulserInfo[pIdx][-1]
        syncChan = wl.getChan(0,10,0) # 672

        runList = cal.GetSpecialRuns("extPulser",pIdx)
        for run in runList:

            # elogs: "20 Hz, 150 second runs"
            gds = GATDataSet(run)
            runTime = gds.GetRunTime() # sec
            # pulseRate = 20 # Hz

            fList = glob.glob(dsi.specialDir+"/lat/latSkimDS0_run%d_*.root" % run)
            tt = TChain("skimTree")
            for f in fList: tt.Add(f)

            tCut = "(channel==%d || channel==%d) && mH==2" % (syncChan, extChan) # enforce correct sync
            n = tt.Draw("trapENFCal:channel:fitSlo",tCut,"goff")
            hitE, chan, fSlo = tt.GetV1(), tt.GetV2(), tt.GetV3()
            hitE = np.asarray([hitE[i] for i in range(n) if chan[i]==extChan])
            fSlo = np.asarray([fSlo[i] for i in range(n) if chan[i]==extChan])

            if len(hitE)==0:
                continue

            extData[run] = [pIdx, runTime, extChan, hitE, fSlo]

            tt.Reset()

    # save output
    # np.savez("./data/lat-extPulser.npz",extData)


def plot_ext_pulser():
    """ The ext pulser data doesn't have the same centroid as the physics data
    since it wasn't tuned quite right.  So we find the centroid of a higher-E set for each channel
    and plot the other ones relative to that. """

    ds, cIdx, bIdx, sIdx = 0, 33, 75, 0 # runs 6887-6963, closest to this one
    # calDB = db.TinyDB('./calDB.json')
    # pars = db.Query()
    # fsD = dsi.getDBRecord("fitSlo_%s_idx%d_m2s238" % ("ds0_m1", cIdx), False, calDB, pars)
    # thD = dsi.getDBRecord("thresh_ds%d_bkg%d_sub%d" % (ds, bIdx, sIdx), False, calDB, pars)

    f = np.load("./data/lat-extPulser.npz")
    extData = f['arr_0'].item()  # {run: [pIdx, runTime, extChan, hitE, fSlo]}

    # get centroids
    # for run in extData:
    #     ch = extData[run][2]
    #     muE = np.mean(extData[run][3])
    #     stdE = np.std(extData[run][3])
    #     muFS = np.mean(extData[run][4])
    #     stdFS = np.std(extData[run][4])
    #     print("run %d  ch %d  E %.2f pm %.2f  FS %.2f pm %.2f" % (run, ch, muE, stdE, muFS, stdFS))
    # run 7236  ch 624  E 54.78 pm 0.14  FS 74.80 pm 1.41
    # run 7249  ch 688  E 46.83 pm 0.11  FS 67.14 pm 1.81
    # run 7220  ch 674  E 71.53 pm 1.30  FS 67.56 pm 95.30
    # runMap = {7236:624, 7249:688, 7220:674}

    # cent = {}
    # for run in extData:
    #     if run not in runMap: continue
    #     ch = extData[run][2]
    #     fLo, fHi, fpb = 50, 100, 0.5
    #     x, hist = wl.GetHisto(extData[run][4], fLo, fHi, fpb)
    #     fMax = x[np.argmax(hist)]
    #     cent[ch] = fMax
    #     # plt.plot(x, hist, ls='steps')
    #     # plt.axvline(fMax, c='r')
    #     # plt.show()

    # this is the result of the above block
    centMap = {624: 74.75, 688: 67.25, 674: 65.75}

    # fig = plt.figure(figsize=(8,6))
    p1 = plt.subplot(111)
    # p2 = plt.subplot(212)

    # hitE, fSlo = [], []
    cols = {624:'r', 688:'g', 674:'b'}
    for run in extData:
        ch = extData[run][2]
        hitE = extData[run][3]
        # fSlo = extData[run][4] # unshifted
        fSlo = [fs - centMap[ch] for fs in extData[run][4]] # shifted
        p1.plot(hitE, fSlo, '.', c=cols[ch], ms=0.5, alpha=0.7)

        fLo, fHi, fpb = -50, 200, 2
        x, hist = wl.GetHisto(fSlo, fLo, fHi, fpb)
        fMax = x[np.argmax(hist)]
        muE = np.mean(hitE)
        # p2.plot(muE, fMax, ".", c=cols[ch], ms=10)

    for ch in cols:
        cpd = det.getChanCPD(ds, ch)
        p1.plot(np.nan, np.nan, '.', c=cols[ch], label="C%sP%sD%s" % (cpd[0],cpd[1],cpd[2]))
        # p2.plot(np.nan, np.nan, '.', c=cols[ch], label="C%sP%sD%s" % (cpd[0],cpd[1],cpd[2]))

    p1.set_ylim(-80, 200) # shifted
    # p1.set_ylim(0, 200) # unshifted

    p1.set_xlim(0, 20)
    # p2.set_xlim(0, 20)
    p1.legend(loc=1)
    p1.set_xlabel("Energy (keV)", ha='right', x=1)
    # p2.set_ylabel("fitSlo Centroid", ha='right', y=1)
    p1.set_ylabel("fitSlo", ha='right', y=1)

    plt.tight_layout()
    # plt.show()
    plt.savefig("./plots/lat-extPulser-centroid.png")


def ext_pulser_width():

    f = np.load("../data/lat-extPulser.npz")
    extData = f['arr_0'].item()  # {run: [pIdx, runTime, extChan, hitE, fSlo]}

    extInfo = {624:[7236, 74.75, 'r'], 688:[7249, 67.25, 'g'], 674:[7220, 65.75, 'b']}

    def xGaussPDF(x,k,loc,scale,amp):
        """ amp=overall multip factor, loc=mu, sc=sigma, k=tau """
        from scipy.stats import exponnorm
        return amp * exponnorm.pdf(x,k,loc,scale)

    def xGaussCDF(x,k,loc,scale,amp):
        from scipy.stats import exponnorm
        return exponnorm.cdf(x,k,loc,scale)

    fsMax, fsLo, fsHi, fsE, fsR = [], [], [], [], []

    xgFits = {}

    for i, run in enumerate(extData):

        ch = extData[run][2]
        cpd = det.getChanCPD(0, ch)

        # if ch != 674: continue
        # if i != 14: continue
        if run in [7233]: continue

        hitE = extData[run][3]
        fSlo = extData[run][4]
        idx = np.where(np.isfinite(hitE))
        hitE, fSlo = hitE[idx], fSlo[idx]

        muE = np.average(hitE)
        sdE = np.std(hitE)
        if muE < 1.: continue
        if muE > 100: continue

        runTime = extData[run][1]/1e9 # elogs: 20 Hz, 150 sec runs
        pulseRate = 20 # Hz
        nExp = int(runTime * pulseRate)
        nTot = len(hitE)
        tEff = nTot / nExp

        # fSlo = extData[run][4] # unshifted
        fSlo = [fs - extInfo[ch][1] for fs in extData[run][4]] # shifted

        fLo, fHi = np.percentile(fSlo,1) * 1.2, np.percentile(fSlo,97) * 1.2
        if fHi > 50: fHi = 50

        nb = 100
        fpb = (fHi-fLo)/nb

        x, h = wl.GetHisto(fSlo, fLo, fHi, fpb, shift=False)
        fMax = x[np.argmax(h)]
        pct = []
        for p in [10, 90]:
            tmp = np.cumsum(h)/np.sum(h)*100
            idx = np.where(tmp > p)
            pct.append(x[idx][0])
        wid = pct[1]-pct[0]

        fsMax.append(fMax)
        fsLo.append(pct[0])
        fsHi.append(pct[1])
        fsE.append(muE)
        fsR.append(run)

        tau, mu, sig, amp = 10, fMax, 10, 10000
        np.seterr(invalid='ignore', over='ignore')
        popt,_ = curve_fit(xGaussPDF, x, h, p0=(tau, mu, sig, amp)) # tau, mu, sig, amp
        np.seterr(invalid='warn', over='warn')
        xgFits[run] = popt

        # print("%d  %d  %d  lo %.2f  hi %.2f  fpb %.2f  E %-5.1f  rt %.1f  %d/%d (%.3f)  FS  10%% %-5.1f  max %-5.1f  90%% %-5.1f  W %.1f" % (i, run, ch, fLo, fHi, fpb, muE, runTime, nTot, nExp, tEff, pct[0], fMax, pct[1], wid))

        # plt.plot(x, h, ls='steps-mid', c=extInfo[ch][2], label="C%sP%sD%s, E %.1f  Max %.1f  Wid %.1f" % \
        #     (cpd[0],cpd[1],cpd[2],muE, fMax,wid))
        # xFunc = np.arange(fLo, fHi, 0.01)
        # tau, mu, sig, amp = popt
        # xgFit = xGaussPDF(xFunc, *popt)
        # xgMax = xFunc[np.argmax(xgFit)]
        # plt.plot(xFunc, xgFit, '-b', label = "xGaus, tau %.1f  mu %.1f  sig %.1f  amp %.1f" % (tau, mu, sig, amp))
        # # get CDF
        # # xgInt = xGaussCDF(xFunc, *popt)
        # # distMax = np.amax(xgFit)
        # # plt.plot(xFunc, xgInt, '-k', label = "xGauss CDF")
        # # plt.axvline(xgMax, c='m', label='xg max %.1f' % xgMax)
        # # plt.axvline(fMax, c='k')
        # plt.axvline(pct[0], c='b')
        # plt.axvline(pct[1], c='g')
        # plt.xlabel("fitSlo (shifted)", ha='right', x=1)
        # plt.legend(loc=1, fontsize=10)
        # plt.tight_layout()
        # plt.show()

        # return

    # ======== plot the fast pulse envelope ========

    fsMax, fsLo, fsHi, fsE, fsR = np.asarray(fsMax), np.asarray(fsLo), np.asarray(fsHi), np.asarray(fsE). np.asarray(fsR)
    idx = np.argsort(fsE)
    fsMax, fsLo, fsHi, fsE = fsMax[idx], fsLo[idx], fsHi[idx], fsE[idx]
    wid = fsHi[-1] - fsLo[-1]
    fsWid = (fsHi-fsLo)/wid

    # for i in range(len(fsE)):
        # print("E %.1f  fMax %.1f  fWid %.1f" % (fsE[i], fsMax[i], fsWid[i]))

    plt.errorbar(fsE, fsMax, yerr=[fsMax-fsLo, fsHi-fsMax], ms=7, lw=3, c='k', fmt='o')
    # plt.errorbar(fsE, fsMax, yerr=fsWid, ms=1, lw=2, c='r', fmt='o')

    def exp1(x, a, b, t): return a * np.exp(x / t) + b
    def exp2(x, a, b, t): return a * (1 - np.exp(x / t)) + b

    poptUP,_ = curve_fit(exp1, fsE, fsHi, p0=(90, 3, -3))
    poptCT,_ = curve_fit(exp2, fsE, fsMax, p0=(100, -100, -3))
    poptLO,_ = curve_fit(exp2, fsE, fsLo, p0=(100, -100, -3))

    xFunc = np.arange(1, 20, 0.01)
    plt.plot(xFunc, exp1(xFunc,*poptUP), "-r")
    plt.plot(xFunc, exp2(xFunc,*poptCT), "-g")
    plt.plot(xFunc, exp2(xFunc,*poptLO), "-r")

    plt.xlim(1, 2)
    plt.xlabel("Energy (keV)", ha='right', x=1)
    plt.ylabel("fitSlo (shifted)", ha='right', y=1)
    plt.tight_layout()
    plt.show()


def plot_tOffset():
    from ROOT import TChain, TFile

    dsList = [0,1,2,3,4,"5A","5B","5C"]

    tt = TChain("skimTree")
    enrExp, natExp = 0, 0
    for ds in dsList:
        inFile = "%s/bkg/cut/final%d/final%d_DS%s.root" % (dsi.dataDir, pctTot, pctTot, ds)
        tf = TFile(inFile)
        enrExp += float(tf.Get("enrExp (kg-d)").GetTitle())
        natExp += float(tf.Get("natExp (kg-d)").GetTitle())
        tf.Close()
        tt.Add(inFile)

    n = tt.Draw("trapENFCal:tOffset:globalTime","trapENFCal < 50","goff")
    hitE, hitT, hitR = tt.GetV1(), tt.GetV2(), tt.GetV3()
    hitE = [hitE[i] for i in range(n)]
    hitT = [hitT[i] for i in range(n)]
    hitR = [hitR[i] for i in range(n)]

    # thesis plot, don't delete
    fig, (p1, p2) = plt.subplots(2, 1, figsize=(8,6))
    xLo, xHi, xpb = 0, 10, 0.2
    yLo, yHi, ypb = 0, 20000, 1000
    nbx, nby = int((xHi-xLo)/xpb), int((yHi-yLo)/ypb)
    p1.hist2d(hitE, hitT, bins=[nbx, nby], range=[[xLo,xHi],[yLo,yHi]], cmap='jet',norm=LogNorm())
    p1.axhline(4000, c='c', label='tOffset cut: 4000')
    p1.legend(loc=1)
    xLo, xHi, xpb = 0, 10, 0.2
    yLo, yHi, ypb = 0, 2000, 100
    nbx, nby = int((xHi-xLo)/xpb), int((yHi-yLo)/ypb)
    p2.hist2d(hitE, hitT, bins=[nbx, nby], range=[[xLo,xHi],[yLo,yHi]], cmap='jet',norm=LogNorm())
    p1.set_ylabel("tOffset", ha='right', y=1)
    p2.set_ylabel("tOffset", ha='right', y=1)
    p2.set_xlabel("Energy (keV)", ha='right', x=1)
    plt.tight_layout()
    plt.show()
    # plt.savefig("./plots/lat-tOffset-vsE.pdf")
    return

    plt.plot(hitR, hitT, '.', ms=3, c='b')
    plt.xlabel("Unix Time (sec)", ha='right', x=1)
    plt.gca().xaxis.set_label_coords(0.93, -0.075)

    plt.ylabel("tOffset (ns)", ha='right', y=1)

    plt.tight_layout()
    # plt.show()
    plt.savefig("./plots/lat-toffset.png")


def plot_riseNoise():

    f = np.load("./data/sea-plt-2.npz")

    hitE, fSlo, chans, runs = f['arr_0'], f['arr_1'], f['arr_2'], f['arr_3']
    fsCut, thMu, thSig = f['arr_4'], f['arr_5'], f['arr_6']
    rise, riseCut = f['arr_7'], f['arr_8']

    # hitPass, rnPass, hitFail, rnFail = [], [], [], []
    # for i in range(len(hitE)):
    #     if chans[i] in [610, 690, 692, 600, 598, 578, 592]:
    #         continue
    #     # if rise[i] < riseCut[i] and fSlo[i] < fsCut[i] and hitE[i] > (thMu[i]+3*thSig[i]):
    #     # if fSlo[i] < fsCut[i] and hitE[i] > (thMu[i]+3*thSig[i]):
    #     # eThresh = thMu[i]+3*thSig[i]
    #     # if hitE[i] > (eThresh):
    #         # print("%d  e %.2f  t %.2f" % (chans[i], hitE[i],eThresh))
    #     if rise[i] < riseCut[i]:
    #         hitPass.append(hitE[i])
    #         rnPass.append(rise[i])
    #     else:
    #         hitFail.append(hitE[i])
    #         rnFail.append(rise[i])

    # hitPass, rnPass = [], []
    # for i in range(len(hitE)):
    #     if chans[i] in [610, 690, 692, 600, 598, 578, 592]:
    #         continue
    #     if rise[i] < riseCut[i] and hitE[i] > (thMu[i]+3*thSig[i]):
    #         rnPass.append(rise[i])
    #         hitPass.append(hitE[i])


    fig = plt.figure()
    xLo, xHi, xpb = 0.5, 20, 0.1
    # plt.semilogy(hitE,rise,".b",ms=5,label='all') # this shows the untagged pulser evts.  whew.

    # for the purposes of the talk, why not just draw a line at 10?
    n = len(hitE)
    hitPass, rnPass, hitFail, rnFail = [], [], [], []
    for i in range(n):
        if rise[i] < 10:
            hitPass.append(hitE[i])
            rnPass.append(rise[i])
        else:
            hitFail.append(hitE[i])
            rnFail.append(rise[i])
    print("loopd")

    plt.semilogy(hitPass,rnPass,".k", ms=5, label='Pass')
    plt.semilogy(hitFail,rnFail,".r", ms=5, label='Fail')
    plt.axhline(10,color='red')

    plt.ylim(0.5, 3e2)
    plt.xlim(0, 30)
    plt.xlabel("Energy (keV)",ha='right',x=1.)
    plt.ylabel("riseNoise",ha='right',y=1.)
    plt.legend(loc=1)
    plt.tight_layout()
    plt.savefig("./plots/lat-rn-simpleCut.png")


def fitSlo_efficiency_uncertainty():
    """
    NOTE: this function actually writes to the calDB
    and saves the weibull parameters for the upper and lower fit efficiency curves.
    Maybe this should be moved to lat2 someday.
    Keys: fitSlo_cpd_effLo95 = {cpd : [-1, -1, ampLo, cLo, locLo, scLo, ampELo, cELo, locELo, scELo]}
          fitSlo_cpd_effHi95 = {cpd : [-1, -1, ampHi, cHi, locHi, scHi, ampEHi, cEHi, locEHi, scEHi]}
    """
    makePlots = True
    writeDB = False

    from statsmodels.stats import proportion
    from scipy.optimize import curve_fit
    from scipy.stats import chisquare
    pctTot = 95
    xPassLo, xPassHi, xpbPass = 0, 50, 1      # "low energy" region

    # weibull fit constraints: energy, (c, loc, scale, amp)
    eFitHi = 30
    # fitBnd = ((0,-15,0,0),(np.inf,np.inf,np.inf,1.)) # this is the original
    fitBnd = ((1,-20,0,0.5),(np.inf,np.inf,np.inf, 0.99)) # eFitHi=30 and these works! << -- main fit

    # load efficiency data
    f = np.load('%s/data/lat2-eff-data-%d.npz' % (dsi.latSWDir, pctTot))
    effData = f['arr_0'].item()

    # ******
    effLimitLo = {}  # store the (pctTot)% vals of the lower efficiency uncertainty
    effLimitHi = {}  # store the (pctTot)% vals of the upper efficiency uncertainty
    # ******

    # print("CPD   amp     c       loc      scale   e1keV  nBin   maxR    chi2")
    detList = det.allDets
    for i, cpd in enumerate(detList):
    # for i, cpd in enumerate(['122', '114', '273', '133']): # these have convergence issues
    # for i, cpd in enumerate(['133']):
    # for i, cpd in enumerate(['163']):

        if cpd not in effData.keys():
            continue

        xEff, sloEff, ci_low, ci_upp = effData[cpd][0], effData[cpd][1], effData[cpd][2], effData[cpd][3]
        hPass, hFail, hTot, xELow = effData[cpd][4], effData[cpd][5], effData[cpd][6], effData[cpd][7]

        # get the average number of counts per bin under 10 kev, both passing and failing
        nBin = np.sum(hPass[np.where(xELow < 10)])/(10/xpbPass)

        # efficiency for low-E region (xPassLo, xPassHi)

        # start by only looking at points where we have >0 hits passing
        idxP = np.where(hPass > 0)
        sloEff = hPass[idxP] / hTot[idxP]
        xEff = xELow[idxP]

        # calculate confidence intervals for each point (error bars)
        ci_low, ci_upp = proportion.proportion_confint(hPass[idxP], hTot[idxP], alpha=0.1, method='beta')

        # limit the energy range
        idxE = np.where((xEff < xPassHi) & (xEff > 1))
        xEff, sloEff, ci_low, ci_upp = xEff[idxE], sloEff[idxE], ci_low[idxE], ci_upp[idxE]

        # get the set of upper and lower points, removing nan's
        sloEffHi = np.asarray([sloEff[i] + (ci_upp[i] - sloEff[i]) for i in range(len(sloEff))]) # upper
        sloEffLo = np.asarray([sloEff[i] - (sloEff[i] - ci_low[i]) for i in range(len(sloEff))]) # lower
        idxHi = np.where(np.isfinite(sloEffHi))
        idxLo = np.where(np.isfinite(sloEffLo))
        xEffHi, sloEffHi = xEff[idxHi], sloEffHi[idxHi]
        xEffLo, sloEffLo = xEff[idxLo], sloEffLo[idxLo]


        # ** this is essential, otherwise the blue dots show up on the right side of
        # where the bin center would be, and throw off the fit by half a bin width
        xEff -= xpbPass/2.

        # limit the fit energy range
        idxF = np.where(xEff <= eFitHi)
        idxFU = np.where(xEffHi <= eFitHi)
        idxFL = np.where(xEffLo <= eFitHi)

        # run the fits
        popt, pcov = curve_fit(wl.weibull, xEff[idxF], sloEff[idxF], bounds=fitBnd) # best
        poptHi, pcovHi = curve_fit(wl.weibull, xEffHi[idxFU], sloEffHi[idxFU], bounds=fitBnd) # upper
        poptLo, pcovLo = curve_fit(wl.weibull, xEffLo[idxFL], sloEffLo[idxFL], bounds=fitBnd) # lower

        # save pars and errors
        perr = np.sqrt(np.diag(pcov))
        c, loc, sc, amp = popt
        cE, locE, scE, ampE = perr
        eff1 = wl.weibull(1.,*popt)

        cLo, locLo, scLo, ampLo = poptLo
        perrLo = np.sqrt(np.diag(pcovLo))
        cELo, locELo, scELo, ampELo = perrLo

        cHi, locHi, scHi, ampHi = poptHi
        perrHi = np.sqrt(np.diag(pcovHi))
        cEHi, locEHi, scEHi, ampEHi = perrHi

        # fill output dict's
        effLimitLo[cpd] = [-1, -1, ampLo, cLo, locLo, scLo, ampELo, cELo, locELo, scELo]
        effLimitHi[cpd] = [-1, -1, ampHi, cHi, locHi, scHi, ampEHi, cEHi, locEHi, scEHi]

        # get residual
        hResidSig = []
        n = len(sloEff)
        for i in range(n):
            diff = wl.weibull(xEff[i], *popt) - sloEff[i]
            sig = ci_upp[i] if diff > 0 else ci_low[i]
            if sig==0:
                # print("zero at",xEff[i],"kev")
                hResidSig.append(0)
                continue
            hResidSig.append(diff/sig)
        hResidSig = np.asarray(hResidSig)

        # find abs. max of residual
        maxR = np.max(np.fabs(hResidSig))

        # "This test is invalid when the observed or expected frequencies in each category are too small. A typical rule is that all of the observed and expected frequencies should be at least 5."
        # wenqin: the bin errors are binomial.  see https://root.cern.ch/doc/master/classTEfficiency.html
        chi2, p = chisquare(sloEff, wl.weibull(xEff,*popt))

        # print results
        print("%s  %-4.3f  %-4.1f  %-7.2f  %-5.2f  %-3.2f  %d  %.3f  %.3f" % (cpd, amp, c, loc, sc, eff1, nBin, maxR, chi2))

        # print latexable results
        # print("%s & %-4.3f & %-4.1f & %-7.2f & %-5.2f & %-3.2f & %-4d & %-5.3f & %-5.3f \\\\" % (cpd, amp, c, loc, sc, eff1, nBin, maxR, chi2))

        if makePlots:

            # === make the efficiency plot, with sigma residual ===
            plt.close()
            fig = plt.figure(3) # efficiency plot
            p1 = plt.subplot2grid((3,1), (0,0), rowspan=2)
            p2 = plt.subplot2grid((3,1), (2,0), sharex=p1)

            # plot efficiency
            p1.cla()
            p1.plot(xEff, sloEff, '.b', ms=10., label='C%sP%sD%s, nBin %.1f' % (cpd[0],cpd[1],cpd[2],nBin))
            p1.errorbar(xEff, sloEff, yerr=[sloEff - ci_low, ci_upp - sloEff], color='k', linewidth=0.8, fmt='none')

            xFunc = np.arange(xPassLo, xPassHi, 0.1)
            p1.plot(xFunc, wl.weibull(xFunc, *popt), 'g-', lw=1, label=r'Weibull CDF')
            p1.axvline(1.,color='b', lw=1., label='1.0 keV efficiency: %.2f' % wl.weibull(1.,*popt))

            # p1.plot(xEff, sloEffLo, '.r', ms=10., label='lower, data')
            p1.plot(xFunc, wl.weibull(xFunc, *poptLo), 'r-', lw=1, label="Lower")
            p1.plot(xFunc, wl.weibull(xFunc, *poptHi), 'r-', lw=1, label="Upper")


            p1.set_xlim(xPassLo, xPassHi)
            p1.set_ylim(0,1)
            # p1.set_xlabel("hitE (keV)", ha='right', x=1)
            p1.set_ylabel("Efficiency", ha='right', y=1)
            p1.yaxis.set_label_coords(-0.095, 1.)
            p1.legend(loc=4)

            # plot residual
            p2.cla()
            p2.plot(xEff, hResidSig, ".g")
            p2.annotate('Residual, Fit - Data', xy=(470, 80), xycoords='axes points', size=14, ha='right', va='center', bbox=dict(boxstyle='round', fc='w', alpha=0.75, edgecolor='gray'))
            p2.axvline(1.,color='b', lw=1.)

            p2.set_xlim(xPassLo, xPassHi)
            yLo = -0.4 if np.min(hResidSig) > -0.4 else np.min(hResidSig) * 1.1
            yHi = 0.4 if np.max(hResidSig) < 0.4 else np.max(hResidSig) * 1.1
            p2.set_ylim(yLo, yHi)

            p2.set_ylabel("Residual (Sigma)")
            p2.set_xlabel("Energy (keV)", ha='right', x=1)
            p2.set_xticks(np.arange(0,51,5))

            plt.tight_layout()
            fig.subplots_adjust(hspace=0.01)
            plt.setp(p1.get_xticklabels(), visible=False)

            # plt.show()
            plt.savefig("%s/plots/effStudy%d-%s-env.pdf" % (dsi.latSWDir, pctTot,cpd))


    # Finally, write the fit function parameters as a separate DB entry for each cpd (all DS's)
    if writeDB:
        dbFile = '%s/calDB-v2.json' % (dsi.latSWDir)
        print("Writing results to DB :",dbFile)
        calDB = db.TinyDB(dbFile)
        pars = db.Query()

        # write the lower value
        dbKey = "fitSlo_cpd_effLo%s" % pctTot
        dbVals = effLimitLo
        dsi.setDBRecord({"key":dbKey, "vals":dbVals}, forceUpdate=True, calDB=calDB, pars=pars)
        print("DB filled:",dbKey)

        # write the upper value
        dbKey = "fitSlo_cpd_effHi%s" % pctTot
        dbVals = effLimitHi
        dsi.setDBRecord({"key":dbKey, "vals":dbVals}, forceUpdate=True, calDB=calDB, pars=pars)
        print("DB filled:",dbKey)


def rateCheckDS():
    """ ideas:
    rate under 5 kev, for enriched and natural, each DS and combination of DS
    module 1 vs module 2, vs det's w/ high 46.5
    the question is, is the 46.5 correlated w/ the excess?

    table 1: rate from 0-5 kev, module 1 and 2, enr and nat, different DS combinations
    """
    from ROOT import TFile, TChain, TTree, gROOT
    gROOT.ProcessLine("gErrorIgnoreLevel = 3001;")

    mod1 = 0 # 0 false, 1 true
    mod2 = 1
    isEnr = False
    xLo, xHi, xpb = 1, 50, 0.5
    eLo, eHi = 1, 5 # rate window
    # eLo, eHi = 20, 40

    print("Mod1 %d  Mod2 %d  isEnr %d  xLo %d  xHi %d  xpb %.2f  eLo %d  eHi %d" % (mod1, mod2, isEnr, xLo, xHi, xpb, eLo, eHi))

    f = np.load("./data/lat-expo-efficiency-all-e%d.npz" % pctTot)
    xEff0 = f['arr_0']
    detEff, detExpo = f['arr_13'].item(), f['arr_14'].item()
    detList = det.allDets

    dsListList = [ # i'm hilarious
        [0],
        [1],
        [2],
        [3],
        [4],
        ["5A"],
        ["5B"],
        ["5C"],
        [0,1,2,3,4,"5A","5B","5C"],
        [1,2,3,4,"5A","5B","5C"],
        [1,2,3,4,"5B","5C"]
    ]

    for dsList in dsListList:

        tt = TChain("skimTree")
        enrExp, natExp = 0, 0
        xEff = xEff0
        enrEff, natEff = np.zeros(len(xEff)), np.zeros(len(xEff))
        thisEff = np.zeros(len(xEff))

        for ds in dsList:
            tt.Add("%s/bkg/cut/final%dt/final%dt_DS%s.root" % (dsi.dataDir, pctTot, pctTot, ds))

            # get exposure (enrExp, natExp) and efficiency (enrEff, natEff)
            for cpd in detList:
                if (cpd[0]=='1' and mod1) or (cpd[0]=='2' and mod2):
                    if det.isEnr(cpd):
                        enrExp += detExpo[ds][cpd]
                        enrEff = np.add(enrEff, detEff[ds][cpd])
                        # print(ds, cpd, "%.2f" % detExpo[ds][cpd], "%.2f" % enrExp)
                    else:
                        natExp += detExpo[ds][cpd]
                        natEff = np.add(natEff, detEff[ds][cpd])
                        # print(ds, cpd, "%.2f" % detExpo[ds][cpd], "%.2f" % natExp)

        # draw the data
        tCut = "isEnr" if isEnr else "!isEnr"
        if mod1 and not mod2: tCut += " && C==1"
        if mod2 and not mod1: tCut += " && C==2"
        n = tt.Draw("trapENFCal",tCut,"goff")
        if n == 0: continue
        hitE = tt.GetV1()
        hitE = [hitE[i] for i in range(n)]
        x, hCts = wl.GetHisto(hitE, xLo, xHi, xpb, shift=False)

        # final efficiency and exposure (thisEff, thisExp)
        thisEff = enrEff if isEnr else natEff
        thisExp = enrExp if isEnr else natExp
        effNorm = (np.amax(thisEff)/365.25) / (thisExp/365.25)
        thisEff = effNorm * np.divide(thisEff, np.amax(thisEff))
        idxE = np.where((xEff <= xHi) & (xEff >= xLo))
        xEff, thisEff = xEff[idxE], thisEff[idxE]
        hSpec = np.divide(hCts, thisExp * xpb) # scale by exposure and binning to get cts/(keV kg d)
        hErr = np.asarray([np.sqrt(hBin /(thisExp * xpb)) for hBin in hSpec]) # statistical error in each bin

        # get the efficiency-corrected rate
        idxE = np.where((xEff >= eLo) & (xEff <= eHi))
        # effCorr = 1 - 1 / ((xEff[1] - xEff[0]) * np.sum(thisEff[idxE]))
        # idxR = np.where((x >= eLo) & (x <= eHi))
        # hRate = np.sum(xpb * hSpec[idxR])/ (eHi-eLo) / effCorr
        # hRateUnc = hRate * np.sqrt(np.sum(hCts[idxR])) / (np.sum(hCts[idxR])) / effCorr

        effCorr = ((xEff[1] - xEff[0]) * np.sum(thisEff[idxE]))
        idxR = np.where((x >= eLo) & (x <= eHi))
        hRate = effCorr * np.sum(xpb * hSpec[idxR])/ (eHi-eLo)
        hRateUnc = hRate * np.sqrt(np.sum(hCts[idxR]))/(np.sum(hCts[idxR]))

        print("%.3f ± %.3f" % (hRate, hRateUnc),  dsList)

        tt.Reset()



def rateCheckDet():
    """ table 2: ds1-5c, detector rate at 46 vs rate in 0-5, marking which ones are enriched """

    from ROOT import TFile, TChain, TTree, gROOT
    gROOT.ProcessLine("gErrorIgnoreLevel = 3001;")

    f = np.load("./data/lat-expo-efficiency-all-e%d.npz" % pctTot)
    xEff0 = f['arr_0']
    detEff, detExpo = f['arr_13'].item(), f['arr_14'].item()
    detList = det.allDets

    dsList = [0,1,2,3,4,"5A","5B","5C"]

    eff = {cpd:np.zeros(len(xEff0)) for cpd in detList}
    expo = {cpd:0 for cpd in detList}

    tt = TChain("skimTree")
    for ds in dsList:
        tt.Add("%s/bkg/cut/final%dt/final%dt_DS%s.root" % (dsi.dataDir, pctTot, pctTot, ds))
        for cpd in detList:
            eff[cpd] = np.add(eff[cpd], detEff[ds][cpd])
            expo[cpd] += detExpo[ds][cpd]

    r5n, r46n, pbn, pbUn, r5Un = [], [], [], [], []
    r5e, r46e, pbe, pbUe, r5Ue = [], [], [], [], []

    # for cpd in detList[:10]:
    for cpd in detList:
        if expo[cpd]==0: continue
        if cpd in ['254']: continue # not enough counts in peak
        isEnr = det.isEnr(cpd)

        # draw the data
        tCut = "C==%s && P==%s && D==%s" % (cpd[0], cpd[1], cpd[2])
        xLo, xHi, xpb = 1, 50, 0.5

        n = tt.Draw("trapENFCal",tCut,"goff")
        if n == 0: continue
        hitE = tt.GetV1()
        hitE = [hitE[i] for i in range(n)]
        x, hCts = wl.GetHisto(hitE, xLo, xHi, xpb, shift=False)

        thisEff = eff[cpd]
        thisExp = expo[cpd]

        effNorm = (np.amax(thisEff)/365.25) / (thisExp/365.25)
        thisEff = effNorm * np.divide(thisEff, np.amax(thisEff))
        idxE = np.where((xEff0 <= xHi) & (xEff0 >= xLo))
        xEff, thisEff = xEff0[idxE], thisEff[idxE]
        hSpec = np.divide(hCts, thisExp * xpb) # scale by exposure and binning to get cts/(keV kg d)
        hErr = np.asarray([np.sqrt(hBin /(thisExp * xpb)) for hBin in hSpec]) # statistical error in each bin

        # get the efficiency-corrected rates

        # 1--5 keV
        eLo, eHi = 1, 5
        idxE = np.where((xEff >= eLo) & (xEff <= eHi))
        effCorr = ((xEff[1] - xEff[0]) * np.sum(thisEff[idxE]))
        idxR = np.where((x >= eLo) & (x <= eHi))
        rate5 = effCorr * np.sum(xpb * hSpec[idxR])/ (eHi-eLo)
        rate5Unc = rate5 * np.sqrt(np.sum(hCts[idxR])) / (np.sum(hCts[idxR]))

        if rate5 < 0: continue

        if isEnr:
            r5e.append(rate5)
            r5Ue.append(rate5Unc)
        else:
            r5n.append(rate5)
            r5Un.append(rate5Unc)

        # 46--47 keV
        eLo, eHi = 45.5, 47.5
        idxE = np.where((xEff >= eLo) & (xEff <= eHi))
        effCorr = ((xEff[1] - xEff[0]) * np.sum(thisEff[idxE]))
        idxR = np.where((x >= eLo) & (x <= eHi))
        rate46 = effCorr * np.sum(xpb * hSpec[idxR])/ (eHi-eLo)
        rate46Unc = rate46 * np.sqrt(np.sum(hCts[idxR])) / (np.sum(hCts[idxR]))

        if isEnr:
            r46e.append(rate46)
        else:
            r46n.append(rate46)

        # try to plot the 46.5 -- maybe do sideband analysis to subtract the bkg, and skip the eff corr (it's 95%)
        tCut = "C==%s && P==%s && D==%s" % (cpd[0], cpd[1], cpd[2])
        xLo, xHi, xpb = 40, 55, 0.2
        n = tt.Draw("trapENFCal",tCut,"goff")
        if n == 0: continue
        hitE = tt.GetV1()
        hitE = [hitE[i] for i in range(n)]
        x, hCts = wl.GetHisto(hitE, xLo, xHi, xpb, shift=False)
        hSpec = np.divide(hCts, thisExp * xpb)

        pLo, pHi = 46, 47

        idxB = np.where((x <= pLo) | (x >= pHi))
        rateBkg = np.sum(xpb * hSpec[idxB]) /  (pLo-xLo + xHi-pHi)
        rateBkgU = rateBkg * np.sqrt(np.sum(hCts[idxB]))/np.sum(hCts[idxB])

        idxP = np.where((x >= pLo) & (x <= pHi))
        ratePk = np.sum(xpb * hSpec[idxP]) / (pHi-pLo)

        ratePkU = ratePk * np.sqrt(np.sum(hCts[idxP]))/np.sum(hCts[idxP])

        pkBkg = ratePk / rateBkg
        pkBkgU = pkBkg * np.sqrt( (ratePkU/ratePk)**2 + (rateBkgU/rateBkg)**2 )

        if isEnr:
            pbe.append(pkBkg)
            pbUe.append(pkBkgU)
        else:
            pbn.append(pkBkg)
            pbUn.append(pkBkgU)

        print("cpd %s  r5: %.3f ± %.3f  r46: %.3f ± %.3f  P %.4f  B %.4f  P/B %.4f" % (cpd, rate5, rate5Unc, rate46, rate46Unc, rateBkg, ratePk, pkBkg))

        # plt.axhline(rateBkg, c='g')
        # plt.axhline(ratePk, c='r')
        # plt.step(x, hSpec)
        # plt.show()
        # exit()



    # plot the rates

    fig, (p1, p2) = plt.subplots(1,2, figsize=(9,5))

    p1.plot(r5e, pbe, ".b", label="Enriched")
    p1.errorbar(r5e, pbe, yerr=pbUe, xerr=r5Ue, color='k', linewidth=0.8, fmt='none')
    p1.set_xlabel(r"$\mathregular{r_{1-5}}$ [cts/kg-d]", ha='right', x=1)
    p1.set_ylabel(r"$\mathregular{r_{46.5}\ /\ r_{bkg}}$", ha='right', y=1)
    p1.legend(loc=1)

    p2.plot(r5n, pbn, ".r", label="Natural")
    p2.errorbar(r5n, pbn, yerr=pbUn, xerr=r5Un, color='k', linewidth=0.8, fmt='none')
    p2.set_xlabel(r"$\mathregular{r_{1-5}}$ [cts/kg-d]", ha='right', x=1)
    p2.set_ylabel(r"$\mathregular{r_{46.5}\ /\ r_{bkg}}$", ha='right', y=1)
    p2.legend(loc=1)

    plt.tight_layout()
    # plt.show()
    plt.savefig("%s/plots/rates46.pdf" % dsi.latSWDir)


def rateSummary():
    """
        Summary of rates for tables:
        Uses spec_summary for dopeness
    """
    dsCombo = [
        [0],
        [1],
        [2],
        [3],
        [4],
        ["5A"],
        ["5B"],
        ["5C"],
        [6],
        [0,1,2,3,4,"5A","5B","5C",6],
        [1,2,3,4,"5A","5B","5C"],
        [1,2,3,4,"5A","5B","5C",6],
        [1,2,3,4,"5B","5C",6]
    ]

    # Can't go all the way to 200 keV cuz of where I calculated the efficiencies
    # I can probably extrapolate to 250 if we want rates to 250
    eWindowList = [[2,5],[5,20],[20,40],[46,47],[50,100],[100,199]]
    rateMode = True
    for dsL in dsCombo:
        print('DS',dsL)
        for dtype in ['enr','nat']:
            print('dtype',dtype)
            for eWindow in eWindowList:
                spec_summary(dsList=dsL, dtype=dtype, rateMode=rateMode, eMin=eWindow[0], eMax=eWindow[1])


def check_spec(dsList=None, dtype='enr', rateMode=False, eMin=20, eMax=40):
    from ROOT import TFile, TChain, TTree

    dtype='enr'

    if eMin < 1:
        print('Cannot currently calculate rates below 1 keV, setting minimum to 1 keV')
        eMin = 1

    if not dsList:
        dsList = [1,2,3,4,"5A","5B","5C",6]

    # xLo, xHi, xpb = 0, 200, 0.4
    xLo, xHi, xpb = 130, 160, 0.2
    # xLo, xHi, xpb = 20, 50, 0.2
    fName = "./plots/lat-ds6-spec-%s-%d-%d.pdf" % (dtype, xLo, xHi)
    legLoc = 2

    tt = TChain("skimTree")
    enrExp, natExp = 0, 0
    # Change exposures to grab from the output file rather than from ROOT files
    for ds in dsList:
        inFile = "%s/bkg/cut/final%dt/final%dt_DS%s.root" % (dsi.dataDir, pctTot, pctTot, ds)
        tf = TFile(inFile)
        # enrExp += float(tf.Get("enrExp (kg-d)").GetTitle())
        # natExp += float(tf.Get("natExp (kg-d)").GetTitle())
        tf.Close()
        tt.Add(inFile)

    # load efficiency correction
    f = np.load("./data/lat-expo-efficiency-all-e%d.npz" % pctTot)
    xEff = f['arr_0']
    totEnrEff, totNatEff = f['arr_1'].item(), f['arr_2'].item()
    enrExpDict, natExpDict = f['arr_3'].item(), f['arr_4'].item()

    detEff = np.zeros(len(xEff))
    for ds in dsList:
        enrExp += enrExpDict[ds]
        natExp += natExpDict[ds]
        if dtype=="enr": detEff += totEnrEff[ds]
        if dtype=="nat": detEff += totNatEff[ds]

    if dtype=="enr":
        detExp = enrExp
        specLabel = "Enriched"
    if dtype=="nat":
        detExp = natExp
        specLabel = "Natural"

    if not rateMode: print("%s Exp: %.2f, ds" % (specLabel, detExp/365.25), dsList)

    # normalize the efficiency
    effNorm = (np.amax(detEff)/365.25) / (detExp/365.25)
    detEff = effNorm * np.divide(detEff, np.amax(detEff))
    idxE = np.where((xEff <= xHi) & (xEff >= xLo))
    xEff, detEff = xEff[idxE], detEff[idxE]

    # make the plot!
    # fig, (p1, p2) = plt.subplots(2,1, figsize=(8,7))
    fig, p1 = plt.subplots(1,1, figsize=(8,5))

    # ==== 1. energy spectrum ====

    if dtype=="enr": tCut = "isEnr"
    if dtype=="nat": tCut = "!isEnr"

    n = tt.Draw("trapENFCal",tCut,"goff")
    hitE = tt.GetV1()
    hitE = [hitE[i] for i in range(n)]
    x, hCts = wl.GetHisto(hitE, xLo, xHi, xpb, shift=False)

    hSpec = np.divide(hCts, detExp * xpb) # scale by exposure and binning to get cts/(keV kg d)
    hErr = np.asarray([np.sqrt(hBin/(detExp*xpb)) for hBin in hSpec]) # statistical error in each bin

    # calculate background index rate (smaller error if you use 1 keV bins)
    idxR = np.where((x>=eMin) & (x<=eMax))

    # Only do this in rateMode
    if xHi >= eMax and rateMode:
        hSpecCorr = hSpec[:-1]/detEff[::int(len(detEff)/(len(hSpec)-1))]
        hRate = np.sum(xpb * hSpec[idxR])/(eMax-eMin)
        hRateUnc = np.sqrt(np.sum(np.square(hErr[idxR]))/(eMax-eMin))
        hRateCorr = np.sum(xpb * hSpecCorr[idxR])/(eMax-eMin)
        hRateCorrUnc = np.sqrt(np.sum(xpb*hCts[idxR]))/np.sum(xpb*hCts[idxR])*hRateCorr
        # print("Rate %d-%d: %.5f ± %.5f, %.5f ± %.5f" % (eMin, eMax, hRate, hRateUnc, hRateCorr, hRateCorrUnc))
        print("%d-%d & %.3f $\pm$ %.3f" % (eMin, eMax, hRateCorr, hRateCorrUnc))
        return

    dsLabel = "DS%s--%s" % (dsList[0], dsList[-1]) if len(dsList) > 1 else "DS%s" % dsList[0]
    p1.plot(x, hSpec, 'b', ls='steps', lw=2, label="%s, %s" % (specLabel, dsLabel))

    p1.plot(np.nan, np.nan, '--r', alpha=0.8, lw=2, label="Eff, %.2f%% at %d keV" % (effNorm*100, xHi))
    # p1.axvline(1, c='g', lw=1, alpha=0.8, label="1.0 keV")
    # p1.axvline(1.5, c='m', lw=1, alpha=0.8, label="1.5 keV")

    p1.set_xlim(xLo, xHi)
    p1.set_xticks(np.arange(xLo, xHi+1, (xHi-xLo)/10))
    p1.set_xlabel("Energy (keV)", ha='right', x=1)
    p1.set_ylabel("Counts/keV-kg-d  (%.2f keV bins)" % xpb, ha='right', y=1)
    # p1.legend(loc=1, fontsize=14, bbox_to_anchor=(0., 0.7, 1, 0.2))
    p1.legend(loc=legLoc, fontsize=14, bbox_to_anchor=(0., 0.7, 1, 0.2))
    # p1.legend(loc=legLoc, fontsize=14)

    p1a = p1.twinx()
    p1a.plot(xEff, detEff, '--r', alpha=0.8, lw=2)
    p1a.set_ylabel('Efficiency', color='r', ha='right', y=1)
    p1a.set_yticks(np.arange(0,1.1, 0.2))
    p1a.tick_params('y', colors='r')

    plt.tight_layout()
    # plt.show()
    plt.savefig(fName)


def getLAT():

    from ROOT import TFile, TTree

    dsList = [0,1,2,3,4,"5A","5B","5C"]

    # histo output for each ds
    xLo, xHi, xpb = 0, 100, 0.1
    nbx = int((xHi-xLo)/xpb)
    xTot = np.arange(xLo, xHi, xpb)
    hTot = {ds:np.zeros(len(xTot)+1) for ds in dsList} # these 100% match w/ wl.GetHisto's output

    for ds in dsList:
        print("Scanning DS-%s" % (str(ds)))

        dsNum = int(ds[0]) if isinstance(ds,str) else ds

        fList = []
        if ds not in ["5A","5B","5C"]:
            fList = glob.glob("%s/latSkimDS%d*" % (dsi.latDir, ds))
        else:
            if ds=="5A": bLo, bHi = 0, 79
            if ds=="5B": bLo, bHi = 80, 112
            if ds=="5C": bLo, bHi = 113, 121
            for bIdx in range(bLo, bHi+1):
                fList.extend(glob.glob("%s/latSkimDS%d_%d_*" % (dsi.latDir, dsNum, bIdx)))

        for idx, f in enumerate(fList):

            fr = np.fabs(100*idx/len(fList) % 10)
            if fr < 0.5:
                print("%d/%d (file %s) %.1f%% done." % (idx, len(fList), f, 100*idx/len(fList)))

            tf = TFile(f)
            tt = tf.Get("skimTree")

            n = tt.Draw("trapENFCal","","goff")
            hitE = tt.GetV1()
            hitE = [hitE[i] for i in range(n)]

            xF, hF = wl.GetHisto(hitE, xLo, xHi, xpb)
            hTot[ds] = np.add(hTot[ds], hF)

            tf.Close()

    np.savez("./data/lat-cutspec.npz", xTot, hTot)


def getCut(cutType):
    """ ./cut-spec.py -c [cutType] """

    fPrefix = {
        "th"  : "%s/th/th" % dsi.cutDir,          # thresholds only.  ok
        # "fs"  : "%s/fs/fs" % dsi.cutDir,        # can't use this one, it was an old version of the 90% cut
        "fr95": "%s/fr95/fr" % dsi.cutDir,        # this is sort of PSA amalgalm, ok
        "frb95"  : "%s/frb95/frb95" % dsi.cutDir  # this is with burst cut, ok
    }

    from ROOT import TFile, TTree

    dsList = [0,1,2,3,4,"5A","5B","5C"]
    # dsList = ["5B"]

    # histo output for each ds
    xLo, xHi, xpb = 0, 100, 0.1
    nbx = int((xHi-xLo)/xpb)
    xTot = np.arange(xLo, xHi, xpb)
    hTot = {ds:np.zeros(len(xTot)+1) for ds in dsList} # these 100% match w/ wl.GetHisto's output

    for ds in dsList:
        print("Scanning DS-%s" % (str(ds)))

        dsNum = int(ds[0]) if isinstance(ds,str) else ds

        fList = []
        if ds not in ["5A","5B","5C"]:
            fList = glob.glob("%s_ds%d_*" % (fPrefix[cutType], dsNum))
        else:
            if ds=="5A": bLo, bHi = 0, 79
            if ds=="5B": bLo, bHi = 80, 112
            if ds=="5C": bLo, bHi = 113, 121
            for bIdx in range(bLo, bHi+1):
                fList.extend(glob.glob("%s_ds%d_%d_*" % (fPrefix[cutType], dsNum, bIdx)))

        for idx, f in enumerate(fList):

            fr = np.fabs(100*idx/len(fList) % 10)
            if fr < 0.5:
                print("%d/%d (file %s) %.1f%% done." % (idx, len(fList), f, 100*idx/len(fList)))

            tf = TFile(f)
            tt = tf.Get("skimTree")

            n = tt.Draw("trapENFCal","","goff")
            hitE = tt.GetV1()
            hitE = [hitE[i] for i in range(n)]

            xF, hF = wl.GetHisto(hitE, xLo, xHi, xpb)
            hTot[ds] = np.add(hTot[ds], hF)

            tf.Close()

    np.savez("./data/lat-cut-%s-spec.npz" % cutType, xTot, hTot)


def getFinal():
    """ ./cut-spec.py -f """

    from ROOT import TFile, TTree

    dsList = [0,1,2,3,4,"5A","5B","5C"]
    # dsList = ["5B"]

    # cutType = "final90"
    cutType = "final95t" # <-- use this one

    # histo output for each ds
    xLo, xHi, xpb = 0, 100, 0.1
    nbx = int((xHi-xLo)/xpb)
    xTot = np.arange(xLo, xHi, xpb)
    hTot = {ds:np.zeros(len(xTot)+1) for ds in dsList} # these 100% match w/ wl.GetHisto's output

    for ds in dsList:

        tf = TFile("%s/%s/%s_DS%s.root" % (dsi.cutDir, cutType, cutType, str(ds)))

        tt = tf.Get("skimTree")
        n = tt.Draw("trapENFCal","","goff")
        hitE = tt.GetV1()
        hitE = [hitE[i] for i in range(n)]

        xF, hF = wl.GetHisto(hitE, xLo, xHi, xpb)
        hTot[ds] = np.add(hTot[ds], hF)

        tf.Close()

    np.savez("./data/lat-cut-%s-spec.npz" % cutType, xTot, hTot)


def plotSpec():

    # this stuff used to be in a routine called cut-spec.py

    # like a genius, i used a ton of different output file names
    # lat :     ~/project/bkg/lat/latSkimDS5_40_16.root
    # th :      ~/project/bkg/cut/th/th_ds0_10_ch592.root
    # rn :      none
    # fs :      ~/project/bkg/cut/fs/fs_ds0_0_ch594.root
    # fr95:     ~/project/bkg/cut/fr95/fr_ds0_0_ch576.root
    # frb90     ~/project/bkg/cut/frb90/frb90_ds0_0_ch576.root
    # frb95:    ~/project/bkg/cut/frb95/frb95_ds0_0_ch576.root
    # final90:  ~/project/bkg/cut/final90/final90_DS1.root
    # final95t: ~/project/bkg/cut/final95t/final95t_DS1.root  ***

    # "lat", "th", "rn", "fs", "fr90", "fr95", "frb90", "frb95", "final90", "final95"
    # type = "final95"

    # "fitSlo","riseNoise","tOffset"
    # par = "fitSlo"

    # which ds's to combine
    # dsList = [0,1,2,3,4,"5A","5B","5C"]

    # for i, opt in enumerate(argv):
    #     if opt=="-g":
    #         getLAT()
    #     if opt=="-c":
    #         getCut(argv[i+1])
    #     if opt=="-f":
    #         getFinal()


    # initial lat
    f_lat = np.load("./data/lat-cutspec.npz")
    xTot, hTotDS_lat = f_lat['arr_0'], f_lat['arr_1'].item()
    hTot_lat = np.zeros(len(hTotDS_lat[0]))
    for ds in hTotDS_lat:
        hTot_lat = np.add(hTot_lat, hTotDS_lat[ds])

    # th
    f_th = np.load("./data/lat-cut-th-spec.npz")
    xTot, hTotDS_th = f_th['arr_0'], f_th['arr_1'].item()
    hTot_th = np.zeros(len(hTotDS_th[0]))
    for ds in hTotDS_th:
        hTot_th = np.add(hTot_th, hTotDS_th[ds])

    # fr95
    f_fr = np.load("./data/lat-cut-fr95-spec.npz")
    xTot, hTotDS_fr = f_fr['arr_0'], f_fr['arr_1'].item()
    hTot_fr = np.zeros(len(hTotDS_fr[0]))
    for ds in hTotDS_fr:
        hTot_fr = np.add(hTot_fr, hTotDS_fr[ds])

    # frb95
    f_frb = np.load("./data/lat-cut-frb95-spec.npz")
    xTot, hTotDS_frb = f_frb['arr_0'], f_frb['arr_1'].item()
    hTot_frb = np.zeros(len(hTotDS_frb[0]))
    for ds in hTotDS_frb:
        hTot_frb = np.add(hTot_frb, hTotDS_frb[ds])

    # final
    f_fin = np.load("./data/lat-cut-final95t-spec.npz")
    xTot, hTotDS_fin = f_fin['arr_0'], f_fin['arr_1'].item()
    hTot_fin = np.zeros(len(hTotDS_fin[0]))
    for ds in hTotDS_fin:
        hTot_fin = np.add(hTot_fin, hTotDS_fin[ds])

    xTot = np.insert(xTot, 0, 0, axis=0)

    # limit energy range
    idx = np.where(xTot < 50)

    # cmap = plt.cm.get_cmap('jet',6)

    plt.semilogy(xTot[idx], hTot_lat[idx], c='k', ls='steps', lw=2, label="LAT Skims")
    plt.semilogy(xTot[idx], hTot_th[idx], c='b', ls='steps', lw=2, label="Threshold Cut")
    plt.semilogy(xTot[idx], hTot_fr[idx], c='g', ls='steps', lw=2, label="PSA Cuts")
    plt.semilogy(xTot[idx], hTot_frb[idx], c='orange', ls='steps', lw=2, label="Burst Cut")
    plt.semilogy(xTot[idx], hTot_fin[idx], c='r', ls='steps', lw=2, label="tOffset Cut, Final Spectrum")

    plt.xlabel("Energy (keV)", ha='right', x=1)
    plt.ylabel("Counts", ha='right', y=1)
    plt.legend()
    plt.tight_layout()
    # plt.show()
    plt.savefig("./plots/lat-specReduction.pdf")



if __name__=="__main__":
    main()
